Sender: LSF System <lsfadmin@e2207>
Subject: Job 44848613: <test> in cluster <euler> Exited

Job <test> was submitted from host <euler10> by user <simondi> in cluster <euler>.
Job was executed on host(s) <10*e2207>, in queue <normal.4h>, as user <simondi> in cluster <euler>.
</cluster/home/simondi> was used as the home directory.
</cluster/home/simondi/PROJECTS/tix-util/tix_notebooks> was used as the working directory.
Started at Results reported on 
Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/env python 
#BSUB -J test
#BSUB -W 04:00 # runtime to request
#BSUB -o test-%J.log # output extra o means overwrite
#BSUB -n 10 # requesting ncores cores
#BSUB -R "span[ptile=10]"
#BSUB -R "rusage[mem=1000]"


# setup the spark paths
import os
os.environ['SPARK_HOME']='/cluster/home/simondi/spark'
os.environ['SPARK_LOCAL_DIRS']=os.environ['__LSF_JOB_TMPDIR__']
os.environ['LOCAL_DIRS']=os.environ['SPARK_LOCAL_DIRS']
os.environ['SPARK_WORKER_DIR']=os.path.join(os.environ['SPARK_LOCAL_DIRS'], 'work')

from sparkhpc import sparkjob
sparkjob.start_cluster('10000M', 
                       cores_per_executor=10, 
                       spark_home='/cluster/home/simondi/spark')

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   0.72 sec.
    Max Memory :                                 15 MB
    Average Memory :                             15.00 MB
    Total Requested Memory :                     10000.00 MB
    Delta Memory :                               9985.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   46 sec.
    Turnaround time :                            150 sec.

The output (if any) follows:

INFO:sparkhpc.sparkjob:master command: /cluster/home/simondi/spark/sbin/start-master.sh
no org.apache.spark.deploy.master.Master to stop
Traceback (most recent call last):
  File "/cluster/home/simondi/anaconda3/envs/p3/lib/python3.5/site-packages/sparkhpc-0.3.post2-py3.5.egg/sparkhpc/sparkjob.py", line 592, in start_cluster
ValueError: not enough values to unpack (expected 2, got 0)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/cluster/shadow/.lsbatch/1497298522.44848613.shell", line 20, in <module>
    spark_home='/cluster/home/simondi/spark')
  File "/cluster/home/simondi/anaconda3/envs/p3/lib/python3.5/site-packages/sparkhpc-0.3.post2-py3.5.egg/sparkhpc/sparkjob.py", line 600, in start_cluster
RuntimeError: Spark master appears to not be starting -- check the logs at: /cluster/home/simondi/spark/logs/spark_master.out
