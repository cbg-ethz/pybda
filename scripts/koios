#!/usr/bin/env python3

# Copyright (C) 2018 Simon Dirmeier
#
# This file is part of koios.
#
# koios is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# koios is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with koios. If not, see <http://www.gnu.org/licenses/>.
#
# @author = 'Simon Dirmeier'
# @email = 'simon.dirmeier@bsse.ethz.ch'

import logging
import click
import pyspark

from koios.factor_analysis import FactorAnalysis

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
frmtr = logging.Formatter(
  '[%(levelname)-1s/%(processName)-1s/%(name)-1s]: %(message)s')


@click.group()
def cli():
    logging.basicConfig(
      format='[%(levelname)-1s/%(processName)-1s/%(name)-1s]: %(message)s')


@cli.command()
@click.argument("file", type=str,)
@click.argument("outpath", type=str)
@click.argument("factors", type=int)
@click.option("maxit", type=int,  default=25, help="Number of iterations.")
def factor_analysis(file, outpath, factors, max_iter):
    """
    Computes a factor analysis with x FACTORS on a tsv FILE and writes to an
    OUTPATH.
    """

    if outpath.endswith("/"):
        outpath = outpath[:-1]
    hdlr = logging.FileHandler(outpath + ".log")
    hdlr.setFormatter(frmtr)
    logger.addHandler(hdlr)

    logger.info("Initializing pyspark session")
    pyspark.StorageLevel(True, True, False, False, 1)
    global spark
    spark = pyspark.sql.SparkSession()\
        .getOrCreate()

    try:
        from koios.io.io import read_tsv
        data = read_tsv(file)
        fl = FactorAnalysis(spark, max_iter=max_iter)
        fit = fl.fit(data, factors)
        fit.write_files(outpath)
    except Exception as e:
        logger.error("Some error: {}".format(str(e)))

    logger.info("Stopping pyspark context")
    spark.stop()


if __name__ == "__main__":
    cli()

