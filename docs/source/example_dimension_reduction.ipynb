{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension reduction\n",
    "\n",
    "\n",
    "Here, we demonstrate how PyBDA can be used for dimension reduction. We use the `iris` data, because we know _how_ we want the different plants to be clustered. We'll use PCA, factor analysis and LDA for the dimension reduction and embed it into a two-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We activate our environment first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pybda) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "source ~/miniconda3/bin/activate pybda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already provided an example how dimension reduction can be used in the `data` folder. It is fairly simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pybda) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "cd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark: spark-submit\n",
      "infile: iris.tsv\n",
      "outfolder: results\n",
      "meta: iris_meta_columns.tsv\n",
      "features: iris_feature_columns.tsv\n",
      "dimension_reduction: pca, factor_analysis, lda\n",
      "n_components: 2\n",
      "response: Species\n",
      "sparkparams:\n",
      "  - \"--driver-memory=1G\"\n",
      "  - \"--executor-memory=1G\"\n",
      "debug: true\n",
      "(pybda) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "cat pybda-usecase-dimred.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the config above we will do the following:\n",
    "\n",
    "* do three dimensionality reductions to two dimensions on the features in `iris_feature_columns.tsv`,\n",
    "* for the LDA use the response variable `Species`,\n",
    "* give the Spark driver 1G of memory and the executor 1G of memory,\n",
    "* write the results to `results`,\n",
    "* print debug information.\n",
    "\n",
    "As can be seen, the effort to implement the three embedings is minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We execute PyBDA like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking command line arguments for method: dimension_reduction\n",
      "\u001b[1;33m Printing rule tree:\n",
      " -> _ (, iris.tsv)\n",
      "\t -> dimension_reduction (iris.tsv, results/2019_08_08/lda_from_iris.tsv)\n",
      "\t -> dimension_reduction (iris.tsv, results/2019_08_08/factor_analysis_from_iris.tsv)\n",
      "\t -> dimension_reduction (iris.tsv, results/2019_08_08/pca_from_iris.tsv)\n",
      "\u001b[0m\n",
      "\u001b[33mBuilding DAG of jobs...\u001b[0m\n",
      "[2019-08-08 23:07:17,255 - WARNING - snakemake.logging]: Building DAG of jobs...\n",
      "\u001b[33mUsing shell: /bin/bash\u001b[0m\n",
      "[2019-08-08 23:07:17,266 - WARNING - snakemake.logging]: Using shell: /bin/bash\n",
      "\u001b[33mProvided cores: 1\u001b[0m\n",
      "[2019-08-08 23:07:17,266 - WARNING - snakemake.logging]: Provided cores: 1\n",
      "\u001b[33mRules claiming more threads will be scaled down.\u001b[0m\n",
      "[2019-08-08 23:07:17,266 - WARNING - snakemake.logging]: Rules claiming more threads will be scaled down.\n",
      "\u001b[33mJob counts:\n",
      "\tcount\tjobs\n",
      "\t1\tfactor_analysis\n",
      "\t1\tlda\n",
      "\t1\tpca\n",
      "\t3\u001b[0m\n",
      "[2019-08-08 23:07:17,267 - WARNING - snakemake.logging]: Job counts:\n",
      "\tcount\tjobs\n",
      "\t1\tfactor_analysis\n",
      "\t1\tlda\n",
      "\t1\tpca\n",
      "\t3\n",
      "\u001b[32m\u001b[0m\n",
      "[2019-08-08 23:07:17,268 - INFO - snakemake.logging]: \n",
      "\u001b[32m[Thu Aug  8 23:07:17 2019]\u001b[0m\n",
      "[2019-08-08 23:07:17,268 - INFO - snakemake.logging]: [Thu Aug  8 23:07:17 2019]\n",
      "\u001b[32mrule lda:\n",
      "    input: iris.tsv\n",
      "    output: results/2019_08_08/lda.tsv, results/2019_08_08/lda-projection.tsv, results/2019_08_08/lda-plot\n",
      "    jobid: 0\u001b[0m\n",
      "[2019-08-08 23:07:17,268 - INFO - snakemake.logging]: rule lda:\n",
      "    input: iris.tsv\n",
      "    output: results/2019_08_08/lda.tsv, results/2019_08_08/lda-projection.tsv, results/2019_08_08/lda-plot\n",
      "    jobid: 0\n",
      "\u001b[32m\u001b[0m\n",
      "[2019-08-08 23:07:17,268 - INFO - snakemake.logging]: \n",
      "\u001b[1;33m Printing rule tree:\n",
      " -> _ (, iris.tsv)\n",
      "\t -> dimension_reduction (iris.tsv, results/2019_08_08/lda_from_iris.tsv)\n",
      "\t -> dimension_reduction (iris.tsv, results/2019_08_08/factor_analysis_from_iris.tsv)\n",
      "\t -> dimension_reduction (iris.tsv, results/2019_08_08/pca_from_iris.tsv)\n",
      "\u001b[0m\n",
      "\u001b[33mJob counts:\n",
      "\tcount\tjobs\n",
      "\t1\tlda\n",
      "\t1\u001b[0m\n",
      "\u001b[1;33m Submitting job spark-submit --master local --driver-memory=1G --executor-memory=1G /home/simon/PROJECTS/pybda/pybda/lda.py 2 iris.tsv iris_feature_columns.tsv Species results/2019_08_08/lda > results/2019_08_08/ldaspark.log \u001b[\u001b[0m\n",
      "\u001b[32m[Thu Aug  8 23:07:59 2019]\u001b[0m\n",
      "[2019-08-08 23:07:59,155 - INFO - snakemake.logging]: [Thu Aug  8 23:07:59 2019]\n",
      "\u001b[32mFinished job 0.\u001b[0m\n",
      "[2019-08-08 23:07:59,156 - INFO - snakemake.logging]: Finished job 0.\n",
      "\u001b[32m1 of 3 steps (33%) done\u001b[0m\n",
      "[2019-08-08 23:07:59,156 - INFO - snakemake.logging]: 1 of 3 steps (33%) done\n",
      "\u001b[32m\u001b[0m\n",
      "[2019-08-08 23:07:59,156 - INFO - snakemake.logging]: \n",
      "\u001b[32m[Thu Aug  8 23:07:59 2019]\u001b[0m\n",
      "[2019-08-08 23:07:59,156 - INFO - snakemake.logging]: [Thu Aug  8 23:07:59 2019]\n",
      "\u001b[32mrule pca:\n",
      "    input: iris.tsv\n",
      "    output: results/2019_08_08/pca_from_iris.tsv, results/2019_08_08/pca_from_iris-loadings.tsv, results/2019_08_08/pca_from_iris-plot\n",
      "    jobid: 1\u001b[0m\n",
      "[2019-08-08 23:07:59,156 - INFO - snakemake.logging]: rule pca:\n",
      "    input: iris.tsv\n",
      "    output: results/2019_08_08/pca_from_iris.tsv, results/2019_08_08/pca_from_iris-loadings.tsv, results/2019_08_08/pca_from_iris-plot\n",
      "    jobid: 1\n",
      "\u001b[32m\u001b[0m\n",
      "[2019-08-08 23:07:59,156 - INFO - snakemake.logging]: \n",
      "\u001b[1;33m Printing rule tree:\n",
      " -> _ (, iris.tsv)\n",
      "\t -> dimension_reduction (iris.tsv, results/2019_08_08/lda_from_iris.tsv)\n",
      "\t -> dimension_reduction (iris.tsv, results/2019_08_08/factor_analysis_from_iris.tsv)\n",
      "\t -> dimension_reduction (iris.tsv, results/2019_08_08/pca_from_iris.tsv)\n",
      "\u001b[0m\n",
      "\u001b[33mJob counts:\n",
      "\tcount\tjobs\n",
      "\t1\tpca\n",
      "\t1\u001b[0m\n",
      "\u001b[1;33m Submitting job spark-submit --master local --driver-memory=1G --executor-memory=1G /home/simon/PROJECTS/pybda/pybda/pca.py 2 iris.tsv iris_feature_columns.tsv results/2019_08_08/pca_from_iris > results/2019_08_08/pca_from_iris-spark.log \u001b[\u001b[0m\n",
      "\u001b[32m[Thu Aug  8 23:08:37 2019]\u001b[0m\n",
      "[2019-08-08 23:08:37,573 - INFO - snakemake.logging]: [Thu Aug  8 23:08:37 2019]\n",
      "\u001b[32mFinished job 1.\u001b[0m\n",
      "[2019-08-08 23:08:37,574 - INFO - snakemake.logging]: Finished job 1.\n",
      "\u001b[32m2 of 3 steps (67%) done\u001b[0m\n",
      "[2019-08-08 23:08:37,574 - INFO - snakemake.logging]: 2 of 3 steps (67%) done\n",
      "\u001b[32m\u001b[0m\n",
      "[2019-08-08 23:08:37,574 - INFO - snakemake.logging]: \n",
      "\u001b[32m[Thu Aug  8 23:08:37 2019]\u001b[0m\n",
      "[2019-08-08 23:08:37,574 - INFO - snakemake.logging]: [Thu Aug  8 23:08:37 2019]\n",
      "\u001b[32mrule factor_analysis:\n",
      "    input: iris.tsv\n",
      "    output: results/2019_08_08/factor_analysis_from_iris.tsv, results/2019_08_08/factor_analysis_from_iris-loadings.tsv, results/2019_08_08/factor_analysis_from_iris-loglik.tsv, results/2019_08_08/factor_analysis_from_iris-plot\n",
      "    jobid: 2\u001b[0m\n",
      "[2019-08-08 23:08:37,575 - INFO - snakemake.logging]: rule factor_analysis:\n",
      "    input: iris.tsv\n",
      "    output: results/2019_08_08/factor_analysis_from_iris.tsv, results/2019_08_08/factor_analysis_from_iris-loadings.tsv, results/2019_08_08/factor_analysis_from_iris-loglik.tsv, results/2019_08_08/factor_analysis_from_iris-plot\n",
      "    jobid: 2\n",
      "\u001b[32m\u001b[0m\n",
      "[2019-08-08 23:08:37,575 - INFO - snakemake.logging]: \n",
      "\u001b[1;33m Printing rule tree:\n",
      " -> _ (, iris.tsv)\n",
      "\t -> dimension_reduction (iris.tsv, results/2019_08_08/lda_from_iris.tsv)\n",
      "\t -> dimension_reduction (iris.tsv, results/2019_08_08/factor_analysis_from_iris.tsv)\n",
      "\t -> dimension_reduction (iris.tsv, results/2019_08_08/pca_from_iris.tsv)\n",
      "\u001b[0m\n",
      "\u001b[33mJob counts:\n",
      "\tcount\tjobs\n",
      "\t1\tfactor_analysis\n",
      "\t1\u001b[0m\n",
      "\u001b[1;33m Submitting job spark-submit --master local --driver-memory=1G --executor-memory=1G /home/simon/PROJECTS/pybda/pybda/factor_analysis.py 2 iris.tsv iris_feature_columns.tsv results/2019_08_08/factor_analysis_from_iris > results/2019_08_08/factor_analysis_from_iris-spark.log \u001b[\u001b[0m\n",
      "\u001b[32m[Thu Aug  8 23:09:28 2019]\u001b[0m\n",
      "[2019-08-08 23:09:28,649 - INFO - snakemake.logging]: [Thu Aug  8 23:09:28 2019]\n",
      "\u001b[32mFinished job 2.\u001b[0m\n",
      "[2019-08-08 23:09:28,649 - INFO - snakemake.logging]: Finished job 2.\n",
      "\u001b[32m3 of 3 steps (100%) done\u001b[0m\n",
      "[2019-08-08 23:09:28,649 - INFO - snakemake.logging]: 3 of 3 steps (100%) done\n",
      "\u001b[33mComplete log: /home/simon/PROJECTS/pybda/data/.snakemake/log/2019-08-08T230717.199260.snakemake.log\u001b[0m\n",
      "[2019-08-08 23:09:28,649 - WARNING - snakemake.logging]: Complete log: /home/simon/PROJECTS/pybda/data/.snakemake/log/2019-08-08T230717.199260.snakemake.log\n",
      "(pybda) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "pybda dimension-reduction pybda-usecase-dimred.config local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the three methods ran, we should check the plots and statistics. Let's see what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(pybda) total 840\n",
      "-rw-rw-r-- 1    190 Aug  8 23:09 factor_analysis_from_iris-loadings.tsv\n",
      "-rw-rw-r-- 1   4881 Aug  8 23:09 factor_analysis_from_iris.log\n",
      "-rw-rw-r-- 1    483 Aug  8 23:09 factor_analysis_from_iris-loglik.tsv\n",
      "drwxrwxr-x 2   4096 Aug  8 23:09 \u001b[0m\u001b[01;34mfactor_analysis_from_iris-plot\u001b[0m\n",
      "-rw-rw-r-- 1 319409 Aug  8 23:09 factor_analysis_from_iris-spark.log\n",
      "-rw-r--r-- 1  12780 Aug  8 23:09 factor_analysis_from_iris.tsv\n",
      "-rw-rw-r-- 1   2812 Aug  8 23:07 lda.log\n",
      "drwxrwxr-x 2   4096 Aug  8 23:07 \u001b[01;34mlda-plot\u001b[0m\n",
      "-rw-rw-r-- 1    346 Aug  8 23:07 lda-projection.tsv\n",
      "-rw-rw-r-- 1 343222 Aug  8 23:07 ldaspark.log\n",
      "-rw-r--r-- 1  12541 Aug  8 23:07 lda.tsv\n",
      "-rw-rw-r-- 1    348 Aug  8 23:08 pca_from_iris-loadings.tsv\n",
      "-rw-rw-r-- 1   2987 Aug  8 23:08 pca_from_iris.log\n",
      "drwxrwxr-x 2   4096 Aug  8 23:08 \u001b[01;34mpca_from_iris-plot\u001b[0m\n",
      "-rw-rw-r-- 1 101682 Aug  8 23:08 pca_from_iris-spark.log\n",
      "-rw-r--r-- 1  12749 Aug  8 23:08 pca_from_iris.tsv\n",
      "(pybda) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "cd results\n",
    "ls -lgG *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be interesting to look at the different embeddings (since we cannot open them from the command line, we load pre-computed plots)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the embedding of the *PCA*:\n",
    "\n",
    "<img src=\"_static/examples/pca.svg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding of the *factor analysis*:\n",
    "\n",
    "<img src=\"_static/examples/fa.svg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the embedding of the *LDA*. Since, LDA needs a response variable to work, when we create a plot, we include this info:\n",
    "\n",
    "<img src=\"_static/examples/lda.svg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyBDA creates many other files and plots. It is, for instance, always important to look at `log` files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-08-08 23:08:01,888 - INFO - pybda.spark_session]: Initializing pyspark session\n",
      "[2019-08-08 23:08:02,889 - INFO - pybda.spark_session]: Config: spark.master, value: local\n",
      "[2019-08-08 23:08:02,890 - INFO - pybda.spark_session]: Config: spark.driver.port, value: 42629\n",
      "[2019-08-08 23:08:02,890 - INFO - pybda.spark_session]: Config: spark.app.id, value: local-1565298482500\n",
      "[2019-08-08 23:08:02,890 - INFO - pybda.spark_session]: Config: spark.driver.memory, value: 1G\n",
      "[2019-08-08 23:08:02,890 - INFO - pybda.spark_session]: Config: spark.rdd.compress, value: True\n",
      "[2019-08-08 23:08:02,890 - INFO - pybda.spark_session]: Config: spark.serializer.objectStreamReset, value: 100\n",
      "[2019-08-08 23:08:02,890 - INFO - pybda.spark_session]: Config: spark.driver.host, value: 192.168.1.33\n",
      "[2019-08-08 23:08:02,890 - INFO - pybda.spark_session]: Config: spark.executor.id, value: driver\n",
      "[2019-08-08 23:08:02,890 - INFO - pybda.spark_session]: Config: spark.submit.deployMode, value: client\n",
      "(pybda) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "head */pca_from_iris.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the Spark `log` file is sometimes important to look at when the methods failed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-08 23:08:00 WARN  Utils:66 - Your hostname, hoto resolves to a loopback address: 127.0.1.1; using 192.168.1.33 instead (on interface wlp2s0)\n",
      "2019-08-08 23:08:00 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "2019-08-08 23:08:00 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2019-08-08 23:08:01 INFO  SparkContext:54 - Running Spark version 2.4.0\n",
      "2019-08-08 23:08:01 INFO  SparkContext:54 - Submitted application: pca.py\n",
      "2019-08-08 23:08:01 INFO  SecurityManager:54 - Changing view acls to: simon\n",
      "2019-08-08 23:08:01 INFO  SecurityManager:54 - Changing modify acls to: simon\n",
      "2019-08-08 23:08:01 INFO  SecurityManager:54 - Changing view acls groups to: \n",
      "2019-08-08 23:08:01 INFO  SecurityManager:54 - Changing modify acls groups to: \n",
      "2019-08-08 23:08:01 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(simon); groups with view permissions: Set(); users  with modify permissions: Set(simon); groups with modify permissions: Set()\n",
      "2019-08-08 23:08:02 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 42629.\n",
      "2019-08-08 23:08:02 INFO  SparkEnv:54 - Registering MapOutputTracker\n",
      "2019-08-08 23:08:02 INFO  SparkEnv:54 - Registering BlockManagerMaster\n",
      "2019-08-08 23:08:02 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2019-08-08 23:08:02 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up\n",
      "2019-08-08 23:08:02 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-e79ad91e-0a17-4fed-8191-ad6b040ce632\n",
      "2019-08-08 23:08:02 INFO  MemoryStore:54 - MemoryStore started with capacity 366.3 MB\n",
      "2019-08-08 23:08:02 INFO  SparkEnv:54 - Registering OutputCommitCoordinator\n",
      "2019-08-08 23:08:02 INFO  log:192 - Logging initialized @2761ms\n",
      "2019-08-08 23:08:02 INFO  Server:351 - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
      "2019-08-08 23:08:02 INFO  Server:419 - Started @2829ms\n",
      "2019-08-08 23:08:02 INFO  AbstractConnector:278 - Started ServerConnector@6943b7b9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "2019-08-08 23:08:02 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@181c130f{/jobs,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@fcaf045{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@74ba40ea{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3d10e1c1{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@658076fe{/stages,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@439bc7b4{/stages/json,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6541c10f{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@240f56fe{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4f2db2c9{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@39ead8e1{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@24c3a31f{/storage,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7aa5a9df{/storage/json,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@58bce7b8{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@37fe019e{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@e9c9725{/environment,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@59202c78{/environment/json,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@669e81c3{/executors,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@212b8e7f{/executors/json,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4be410c0{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@13ffa628{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1fdbd0d{/static,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@328f700a{/,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@f7a7a1e{/api,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@cdc86ab{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@78cd9c68{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://192.168.1.33:4040\n",
      "2019-08-08 23:08:02 INFO  Executor:54 - Starting executor ID driver on host localhost\n",
      "2019-08-08 23:08:02 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36883.\n",
      "2019-08-08 23:08:02 INFO  NettyBlockTransferService:54 - Server created on 192.168.1.33:36883\n",
      "2019-08-08 23:08:02 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2019-08-08 23:08:02 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 192.168.1.33, 36883, None)\n",
      "2019-08-08 23:08:02 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 192.168.1.33:36883 with 366.3 MB RAM, BlockManagerId(driver, 192.168.1.33, 36883, None)\n",
      "2019-08-08 23:08:02 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 192.168.1.33, 36883, None)\n",
      "2019-08-08 23:08:02 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 192.168.1.33, 36883, None)\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6060cc89{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/simon/PROJECTS/pybda/data/spark-warehouse').\n",
      "2019-08-08 23:08:02 INFO  SharedState:54 - Warehouse path is 'file:/home/simon/PROJECTS/pybda/data/spark-warehouse'.\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@30e6ff57{/SQL,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@db414ac{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7779a5f{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@292cc875{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:02 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3c3bfe9f{/static/sql,null,AVAILABLE,@Spark}\n",
      "2019-08-08 23:08:03 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint\n",
      "2019-08-08 23:08:05 INFO  FileSourceStrategy:54 - Pruning directories with: \n",
      "2019-08-08 23:08:05 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
      "2019-08-08 23:08:05 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>\n",
      "2019-08-08 23:08:05 INFO  FileSourceScanExec:54 - Pushed Filters: \n",
      "2019-08-08 23:08:05 INFO  CodeGenerator:54 - Code generated in 142.955405 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-08 23:08:05 INFO  CodeGenerator:54 - Code generated in 15.579257 ms\n",
      "2019-08-08 23:08:05 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 285.0 KB, free 366.0 MB)\n",
      "2019-08-08 23:08:05 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.4 KB, free 366.0 MB)\n",
      "2019-08-08 23:08:05 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 192.168.1.33:36883 (size: 23.4 KB, free: 366.3 MB)\n",
      "2019-08-08 23:08:05 INFO  SparkContext:54 - Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0\n",
      "2019-08-08 23:08:05 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4201044 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2019-08-08 23:08:05 INFO  SparkContext:54 - Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "2019-08-08 23:08:05 INFO  DAGScheduler:54 - Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2019-08-08 23:08:05 INFO  DAGScheduler:54 - Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)\n",
      "2019-08-08 23:08:05 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "2019-08-08 23:08:05 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "2019-08-08 23:08:05 INFO  DAGScheduler:54 - Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2019-08-08 23:08:05 INFO  MemoryStore:54 - Block broadcast_1 stored as values in memory (estimated size 8.8 KB, free 366.0 MB)\n",
      "2019-08-08 23:08:05 INFO  MemoryStore:54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KB, free 366.0 MB)\n",
      "2019-08-08 23:08:05 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on 192.168.1.33:36883 (size: 4.5 KB, free: 366.3 MB)\n",
      "2019-08-08 23:08:05 INFO  SparkContext:54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1161\n",
      "2019-08-08 23:08:05 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2019-08-08 23:08:05 INFO  TaskSchedulerImpl:54 - Adding task set 0.0 with 1 tasks\n",
      "2019-08-08 23:08:06 INFO  TaskSetManager:54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8325 bytes)\n",
      "2019-08-08 23:08:06 INFO  Executor:54 - Running task 0.0 in stage 0.0 (TID 0)\n",
      "2019-08-08 23:08:06 INFO  FileScanRDD:54 - Reading File path: file:///home/simon/PROJECTS/pybda/data/iris.tsv, range: 0-6740, partition values: [empty row]\n",
      "2019-08-08 23:08:06 INFO  CodeGenerator:54 - Code generated in 8.443064 ms\n",
      "2019-08-08 23:08:06 INFO  Executor:54 - Finished task 0.0 in stage 0.0 (TID 0). 1312 bytes result sent to driver\n",
      "2019-08-08 23:08:06 INFO  TaskSetManager:54 - Finished task 0.0 in stage 0.0 (TID 0) in 125 ms on localhost (executor driver) (1/1)\n",
      "2019-08-08 23:08:06 INFO  TaskSchedulerImpl:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "2019-08-08 23:08:06 INFO  DAGScheduler:54 - ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.212 s\n",
      "2019-08-08 23:08:06 INFO  DAGScheduler:54 - Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.256359 s\n",
      "2019-08-08 23:08:06 INFO  FileSourceStrategy:54 - Pruning directories with: \n",
      "2019-08-08 23:08:06 INFO  FileSourceStrategy:54 - Post-Scan Filters: \n",
      "2019-08-08 23:08:06 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>\n",
      "2019-08-08 23:08:06 INFO  FileSourceScanExec:54 - Pushed Filters: \n",
      "2019-08-08 23:08:06 INFO  CodeGenerator:54 - Code generated in 8.516751 ms\n",
      "2019-08-08 23:08:06 INFO  MemoryStore:54 - Block broadcast_2 stored as values in memory (estimated size 285.0 KB, free 365.7 MB)\n",
      "2019-08-08 23:08:06 INFO  MemoryStore:54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.4 KB, free 365.7 MB)\n",
      "2019-08-08 23:08:06 INFO  BlockManagerInfo:54 - Added broadcast_2_piece0 in memory on 192.168.1.33:36883 (size: 23.4 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:06 INFO  SparkContext:54 - Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0\n",
      "2019-08-08 23:08:06 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4201044 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2019-08-08 23:08:06 INFO  FileSourceStrategy:54 - Pruning directories with: \n",
      "2019-08-08 23:08:06 INFO  FileSourceStrategy:54 - Post-Scan Filters: \n",
      "2019-08-08 23:08:06 INFO  FileSourceStrategy:54 - Output Data Schema: struct<slength: string, swidth: string, plength: string, pwidth: string ... 2 more fields>\n",
      "2019-08-08 23:08:06 INFO  FileSourceScanExec:54 - Pushed Filters: \n",
      "2019-08-08 23:08:06 INFO  CodeGenerator:54 - Code generated in 23.032169 ms\n",
      "2019-08-08 23:08:06 INFO  MemoryStore:54 - Block broadcast_3 stored as values in memory (estimated size 285.0 KB, free 365.4 MB)\n",
      "2019-08-08 23:08:06 INFO  MemoryStore:54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 23.4 KB, free 365.4 MB)\n",
      "2019-08-08 23:08:06 INFO  BlockManagerInfo:54 - Added broadcast_3_piece0 in memory on 192.168.1.33:36883 (size: 23.4 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:06 INFO  SparkContext:54 - Created broadcast 3 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2019-08-08 23:08:06 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4201044 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2019-08-08 23:08:06 INFO  ContextCleaner:54 - Cleaned accumulator 14\n",
      "2019-08-08 23:08:06 INFO  ContextCleaner:54 - Cleaned accumulator 26\n",
      "2019-08-08 23:08:06 INFO  ContextCleaner:54 - Cleaned accumulator 18\n",
      "2019-08-08 23:08:06 INFO  ContextCleaner:54 - Cleaned accumulator 21\n",
      "2019-08-08 23:08:06 INFO  ContextCleaner:54 - Cleaned accumulator 27\n",
      "2019-08-08 23:08:06 INFO  ContextCleaner:54 - Cleaned accumulator 9\n",
      "2019-08-08 23:08:06 INFO  ContextCleaner:54 - Cleaned accumulator 19\n",
      "2019-08-08 23:08:06 INFO  ContextCleaner:54 - Cleaned accumulator 30\n",
      "2019-08-08 23:08:06 INFO  ContextCleaner:54 - Cleaned accumulator 20\n",
      "2019-08-08 23:08:06 INFO  ContextCleaner:54 - Cleaned accumulator 17\n",
      "2019-08-08 23:08:06 INFO  ContextCleaner:54 - Cleaned accumulator 15\n",
      "2019-08-08 23:08:06 INFO  ContextCleaner:54 - Cleaned accumulator 31\n",
      "2019-08-08 23:08:06 INFO  ContextCleaner:54 - Cleaned accumulator 25\n",
      "2019-08-08 23:08:06 INFO  ContextCleaner:54 - Cleaned accumulator 24\n",
      "2019-08-08 23:08:06 INFO  ContextCleaner:54 - Cleaned accumulator 23\n",
      "2019-08-08 23:08:06 INFO  ContextCleaner:54 - Cleaned accumulator 7\n",
      "2019-08-08 23:08:06 INFO  ContextCleaner:54 - Cleaned accumulator 29\n",
      "2019-08-08 23:08:06 INFO  ContextCleaner:54 - Cleaned accumulator 8\n",
      "2019-08-08 23:08:06 INFO  ContextCleaner:54 - Cleaned accumulator 10\n",
      "2019-08-08 23:08:06 INFO  ContextCleaner:54 - Cleaned accumulator 13\n",
      "2019-08-08 23:08:06 INFO  ContextCleaner:54 - Cleaned accumulator 11\n",
      "2019-08-08 23:08:06 INFO  ContextCleaner:54 - Cleaned accumulator 16\n",
      "2019-08-08 23:08:06 INFO  ContextCleaner:54 - Cleaned accumulator 12\n",
      "2019-08-08 23:08:06 INFO  ContextCleaner:54 - Cleaned accumulator 28\n",
      "2019-08-08 23:08:06 INFO  BlockManagerInfo:54 - Removed broadcast_1_piece0 on 192.168.1.33:36883 in memory (size: 4.5 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:06 INFO  ContextCleaner:54 - Cleaned accumulator 22\n",
      "2019-08-08 23:08:06 INFO  SparkContext:54 - Starting job: treeAggregate at RowMatrix.scala:419\n",
      "2019-08-08 23:08:06 INFO  DAGScheduler:54 - Got job 1 (treeAggregate at RowMatrix.scala:419) with 1 output partitions\n",
      "2019-08-08 23:08:06 INFO  DAGScheduler:54 - Final stage: ResultStage 1 (treeAggregate at RowMatrix.scala:419)\n",
      "2019-08-08 23:08:06 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "2019-08-08 23:08:06 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "2019-08-08 23:08:06 INFO  DAGScheduler:54 - Submitting ResultStage 1 (MapPartitionsRDD[15] at treeAggregate at RowMatrix.scala:419), which has no missing parents\n",
      "2019-08-08 23:08:06 INFO  MemoryStore:54 - Block broadcast_4 stored as values in memory (estimated size 20.1 KB, free 365.4 MB)\n",
      "2019-08-08 23:08:06 INFO  MemoryStore:54 - Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.3 KB, free 365.4 MB)\n",
      "2019-08-08 23:08:06 INFO  BlockManagerInfo:54 - Added broadcast_4_piece0 in memory on 192.168.1.33:36883 (size: 10.3 KB, free: 366.2 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-08 23:08:06 INFO  SparkContext:54 - Created broadcast 4 from broadcast at DAGScheduler.scala:1161\n",
      "2019-08-08 23:08:06 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[15] at treeAggregate at RowMatrix.scala:419) (first 15 tasks are for partitions Vector(0))\n",
      "2019-08-08 23:08:06 INFO  TaskSchedulerImpl:54 - Adding task set 1.0 with 1 tasks\n",
      "2019-08-08 23:08:06 INFO  TaskSetManager:54 - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 8325 bytes)\n",
      "2019-08-08 23:08:06 INFO  Executor:54 - Running task 0.0 in stage 1.0 (TID 1)\n",
      "2019-08-08 23:08:07 INFO  FileScanRDD:54 - Reading File path: file:///home/simon/PROJECTS/pybda/data/iris.tsv, range: 0-6740, partition values: [empty row]\n",
      "2019-08-08 23:08:07 INFO  CodeGenerator:54 - Code generated in 10.110679 ms\n",
      "2019-08-08 23:08:07 INFO  PythonRunner:54 - Times: total = 573, boot = 340, init = 230, finish = 3\n",
      "2019-08-08 23:08:07 INFO  Executor:54 - Finished task 0.0 in stage 1.0 (TID 1). 2463 bytes result sent to driver\n",
      "2019-08-08 23:08:07 INFO  TaskSetManager:54 - Finished task 0.0 in stage 1.0 (TID 1) in 605 ms on localhost (executor driver) (1/1)\n",
      "2019-08-08 23:08:07 INFO  TaskSchedulerImpl:54 - Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "2019-08-08 23:08:07 INFO  PythonAccumulatorV2:54 - Connected to AccumulatorServer at host: 127.0.0.1 port: 56645\n",
      "2019-08-08 23:08:07 INFO  DAGScheduler:54 - ResultStage 1 (treeAggregate at RowMatrix.scala:419) finished in 0.615 s\n",
      "2019-08-08 23:08:07 INFO  DAGScheduler:54 - Job 1 finished: treeAggregate at RowMatrix.scala:419, took 0.618580 s\n",
      "2019-08-08 23:08:07 INFO  SparkContext:54 - Starting job: count at /home/simon/PROJECTS/pybda/pybda/stats/stats.py:134\n",
      "2019-08-08 23:08:07 INFO  DAGScheduler:54 - Got job 2 (count at /home/simon/PROJECTS/pybda/pybda/stats/stats.py:134) with 1 output partitions\n",
      "2019-08-08 23:08:07 INFO  DAGScheduler:54 - Final stage: ResultStage 2 (count at /home/simon/PROJECTS/pybda/pybda/stats/stats.py:134)\n",
      "2019-08-08 23:08:07 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "2019-08-08 23:08:07 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "2019-08-08 23:08:07 INFO  DAGScheduler:54 - Submitting ResultStage 2 (PythonRDD[16] at count at /home/simon/PROJECTS/pybda/pybda/stats/stats.py:134), which has no missing parents\n",
      "2019-08-08 23:08:07 INFO  MemoryStore:54 - Block broadcast_5 stored as values in memory (estimated size 20.2 KB, free 365.3 MB)\n",
      "2019-08-08 23:08:07 INFO  MemoryStore:54 - Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.7 KB, free 365.3 MB)\n",
      "2019-08-08 23:08:07 INFO  BlockManagerInfo:54 - Added broadcast_5_piece0 in memory on 192.168.1.33:36883 (size: 10.7 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:07 INFO  SparkContext:54 - Created broadcast 5 from broadcast at DAGScheduler.scala:1161\n",
      "2019-08-08 23:08:07 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 2 (PythonRDD[16] at count at /home/simon/PROJECTS/pybda/pybda/stats/stats.py:134) (first 15 tasks are for partitions Vector(0))\n",
      "2019-08-08 23:08:07 INFO  TaskSchedulerImpl:54 - Adding task set 2.0 with 1 tasks\n",
      "2019-08-08 23:08:07 INFO  TaskSetManager:54 - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 8325 bytes)\n",
      "2019-08-08 23:08:07 INFO  Executor:54 - Running task 0.0 in stage 2.0 (TID 2)\n",
      "2019-08-08 23:08:07 INFO  FileScanRDD:54 - Reading File path: file:///home/simon/PROJECTS/pybda/data/iris.tsv, range: 0-6740, partition values: [empty row]\n",
      "2019-08-08 23:08:07 INFO  PythonRunner:54 - Times: total = 27, boot = -54, init = 79, finish = 2\n",
      "2019-08-08 23:08:07 INFO  Executor:54 - Finished task 0.0 in stage 2.0 (TID 2). 1828 bytes result sent to driver\n",
      "2019-08-08 23:08:07 INFO  TaskSetManager:54 - Finished task 0.0 in stage 2.0 (TID 2) in 39 ms on localhost (executor driver) (1/1)\n",
      "2019-08-08 23:08:07 INFO  TaskSchedulerImpl:54 - Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "2019-08-08 23:08:07 INFO  DAGScheduler:54 - ResultStage 2 (count at /home/simon/PROJECTS/pybda/pybda/stats/stats.py:134) finished in 0.046 s\n",
      "2019-08-08 23:08:07 INFO  DAGScheduler:54 - Job 2 finished: count at /home/simon/PROJECTS/pybda/pybda/stats/stats.py:134, took 0.048937 s\n",
      "2019-08-08 23:08:07 INFO  SparkContext:54 - Starting job: first at RowMatrix.scala:61\n",
      "2019-08-08 23:08:07 INFO  DAGScheduler:54 - Got job 3 (first at RowMatrix.scala:61) with 1 output partitions\n",
      "2019-08-08 23:08:07 INFO  DAGScheduler:54 - Final stage: ResultStage 3 (first at RowMatrix.scala:61)\n",
      "2019-08-08 23:08:07 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "2019-08-08 23:08:07 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "2019-08-08 23:08:07 INFO  DAGScheduler:54 - Submitting ResultStage 3 (MapPartitionsRDD[18] at mapPartitions at PythonMLLibAPI.scala:1346), which has no missing parents\n",
      "2019-08-08 23:08:07 INFO  MemoryStore:54 - Block broadcast_6 stored as values in memory (estimated size 19.7 KB, free 365.3 MB)\n",
      "2019-08-08 23:08:07 INFO  MemoryStore:54 - Block broadcast_6_piece0 stored as bytes in memory (estimated size 10.4 KB, free 365.3 MB)\n",
      "2019-08-08 23:08:07 INFO  BlockManagerInfo:54 - Added broadcast_6_piece0 in memory on 192.168.1.33:36883 (size: 10.4 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:07 INFO  SparkContext:54 - Created broadcast 6 from broadcast at DAGScheduler.scala:1161\n",
      "2019-08-08 23:08:07 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at mapPartitions at PythonMLLibAPI.scala:1346) (first 15 tasks are for partitions Vector(0))\n",
      "2019-08-08 23:08:07 INFO  TaskSchedulerImpl:54 - Adding task set 3.0 with 1 tasks\n",
      "2019-08-08 23:08:07 INFO  TaskSetManager:54 - Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 8325 bytes)\n",
      "2019-08-08 23:08:07 INFO  Executor:54 - Running task 0.0 in stage 3.0 (TID 3)\n",
      "2019-08-08 23:08:07 INFO  FileScanRDD:54 - Reading File path: file:///home/simon/PROJECTS/pybda/data/iris.tsv, range: 0-6740, partition values: [empty row]\n",
      "2019-08-08 23:08:07 INFO  Executor:54 - Finished task 0.0 in stage 3.0 (TID 3). 1896 bytes result sent to driver\n",
      "2019-08-08 23:08:07 INFO  TaskSetManager:54 - Finished task 0.0 in stage 3.0 (TID 3) in 68 ms on localhost (executor driver) (1/1)\n",
      "2019-08-08 23:08:07 INFO  TaskSchedulerImpl:54 - Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "2019-08-08 23:08:07 INFO  DAGScheduler:54 - ResultStage 3 (first at RowMatrix.scala:61) finished in 0.074 s\n",
      "2019-08-08 23:08:07 INFO  DAGScheduler:54 - Job 3 finished: first at RowMatrix.scala:61, took 0.076813 s\n",
      "2019-08-08 23:08:07 INFO  SparkContext:54 - Starting job: treeAggregate at RowMatrix.scala:122\n",
      "2019-08-08 23:08:07 INFO  DAGScheduler:54 - Got job 4 (treeAggregate at RowMatrix.scala:122) with 1 output partitions\n",
      "2019-08-08 23:08:07 INFO  DAGScheduler:54 - Final stage: ResultStage 4 (treeAggregate at RowMatrix.scala:122)\n",
      "2019-08-08 23:08:07 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "2019-08-08 23:08:07 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "2019-08-08 23:08:07 INFO  DAGScheduler:54 - Submitting ResultStage 4 (MapPartitionsRDD[19] at treeAggregate at RowMatrix.scala:122), which has no missing parents\n",
      "2019-08-08 23:08:07 INFO  MemoryStore:54 - Block broadcast_7 stored as values in memory (estimated size 20.8 KB, free 365.3 MB)\n",
      "2019-08-08 23:08:07 INFO  MemoryStore:54 - Block broadcast_7_piece0 stored as bytes in memory (estimated size 10.8 KB, free 365.3 MB)\n",
      "2019-08-08 23:08:07 INFO  BlockManagerInfo:54 - Added broadcast_7_piece0 in memory on 192.168.1.33:36883 (size: 10.8 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:07 INFO  SparkContext:54 - Created broadcast 7 from broadcast at DAGScheduler.scala:1161\n",
      "2019-08-08 23:08:07 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at treeAggregate at RowMatrix.scala:122) (first 15 tasks are for partitions Vector(0))\n",
      "2019-08-08 23:08:07 INFO  TaskSchedulerImpl:54 - Adding task set 4.0 with 1 tasks\n",
      "2019-08-08 23:08:07 INFO  TaskSetManager:54 - Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 8325 bytes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-08 23:08:07 INFO  Executor:54 - Running task 0.0 in stage 4.0 (TID 4)\n",
      "2019-08-08 23:08:07 INFO  FileScanRDD:54 - Reading File path: file:///home/simon/PROJECTS/pybda/data/iris.tsv, range: 0-6740, partition values: [empty row]\n",
      "2019-08-08 23:08:08 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2019-08-08 23:08:08 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "2019-08-08 23:08:08 INFO  PythonRunner:54 - Times: total = 227, boot = 3, init = 221, finish = 3\n",
      "2019-08-08 23:08:08 INFO  Executor:54 - Finished task 0.0 in stage 4.0 (TID 4). 1999 bytes result sent to driver\n",
      "2019-08-08 23:08:08 INFO  TaskSetManager:54 - Finished task 0.0 in stage 4.0 (TID 4) in 243 ms on localhost (executor driver) (1/1)\n",
      "2019-08-08 23:08:08 INFO  TaskSchedulerImpl:54 - Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "2019-08-08 23:08:08 INFO  DAGScheduler:54 - ResultStage 4 (treeAggregate at RowMatrix.scala:122) finished in 0.250 s\n",
      "2019-08-08 23:08:08 INFO  DAGScheduler:54 - Job 4 finished: treeAggregate at RowMatrix.scala:122, took 0.253082 s\n",
      "2019-08-08 23:08:08 WARN  LAPACK:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "2019-08-08 23:08:08 WARN  LAPACK:61 - Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n",
      "2019-08-08 23:08:08 INFO  SparkContext:54 - Starting job: count at RowMatrix.scala:75\n",
      "2019-08-08 23:08:08 INFO  DAGScheduler:54 - Got job 5 (count at RowMatrix.scala:75) with 1 output partitions\n",
      "2019-08-08 23:08:08 INFO  DAGScheduler:54 - Final stage: ResultStage 5 (count at RowMatrix.scala:75)\n",
      "2019-08-08 23:08:08 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "2019-08-08 23:08:08 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "2019-08-08 23:08:08 INFO  DAGScheduler:54 - Submitting ResultStage 5 (MapPartitionsRDD[18] at mapPartitions at PythonMLLibAPI.scala:1346), which has no missing parents\n",
      "2019-08-08 23:08:08 INFO  MemoryStore:54 - Block broadcast_8 stored as values in memory (estimated size 19.5 KB, free 365.3 MB)\n",
      "2019-08-08 23:08:08 INFO  MemoryStore:54 - Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.3 KB, free 365.2 MB)\n",
      "2019-08-08 23:08:08 INFO  BlockManagerInfo:54 - Added broadcast_8_piece0 in memory on 192.168.1.33:36883 (size: 10.3 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:08 INFO  SparkContext:54 - Created broadcast 8 from broadcast at DAGScheduler.scala:1161\n",
      "2019-08-08 23:08:08 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[18] at mapPartitions at PythonMLLibAPI.scala:1346) (first 15 tasks are for partitions Vector(0))\n",
      "2019-08-08 23:08:08 INFO  TaskSchedulerImpl:54 - Adding task set 5.0 with 1 tasks\n",
      "2019-08-08 23:08:08 INFO  TaskSetManager:54 - Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 8325 bytes)\n",
      "2019-08-08 23:08:08 INFO  Executor:54 - Running task 0.0 in stage 5.0 (TID 5)\n",
      "2019-08-08 23:08:08 INFO  FileScanRDD:54 - Reading File path: file:///home/simon/PROJECTS/pybda/data/iris.tsv, range: 0-6740, partition values: [empty row]\n",
      "2019-08-08 23:08:08 INFO  PythonRunner:54 - Times: total = 13, boot = -249, init = 259, finish = 3\n",
      "2019-08-08 23:08:08 INFO  Executor:54 - Finished task 0.0 in stage 5.0 (TID 5). 1893 bytes result sent to driver\n",
      "2019-08-08 23:08:08 INFO  TaskSetManager:54 - Finished task 0.0 in stage 5.0 (TID 5) in 21 ms on localhost (executor driver) (1/1)\n",
      "2019-08-08 23:08:08 INFO  TaskSchedulerImpl:54 - Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "2019-08-08 23:08:08 INFO  DAGScheduler:54 - ResultStage 5 (count at RowMatrix.scala:75) finished in 0.028 s\n",
      "2019-08-08 23:08:08 INFO  DAGScheduler:54 - Job 5 finished: count at RowMatrix.scala:75, took 0.030672 s\n",
      "2019-08-08 23:08:08 INFO  MemoryStore:54 - Block broadcast_9 stored as values in memory (estimated size 104.0 B, free 365.2 MB)\n",
      "2019-08-08 23:08:08 INFO  MemoryStore:54 - Block broadcast_9_piece0 stored as bytes in memory (estimated size 133.0 B, free 365.2 MB)\n",
      "2019-08-08 23:08:08 INFO  BlockManagerInfo:54 - Added broadcast_9_piece0 in memory on 192.168.1.33:36883 (size: 133.0 B, free: 366.2 MB)\n",
      "2019-08-08 23:08:08 INFO  SparkContext:54 - Created broadcast 9 from broadcast at RowMatrix.scala:442\n",
      "2019-08-08 23:08:08 INFO  SparkContext:54 - Starting job: runJob at PythonRDD.scala:153\n",
      "2019-08-08 23:08:08 INFO  DAGScheduler:54 - Got job 6 (runJob at PythonRDD.scala:153) with 1 output partitions\n",
      "2019-08-08 23:08:08 INFO  DAGScheduler:54 - Final stage: ResultStage 6 (runJob at PythonRDD.scala:153)\n",
      "2019-08-08 23:08:08 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "2019-08-08 23:08:08 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "2019-08-08 23:08:08 INFO  DAGScheduler:54 - Submitting ResultStage 6 (PythonRDD[22] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "2019-08-08 23:08:08 INFO  MemoryStore:54 - Block broadcast_10 stored as values in memory (estimated size 22.4 KB, free 365.2 MB)\n",
      "2019-08-08 23:08:08 INFO  MemoryStore:54 - Block broadcast_10_piece0 stored as bytes in memory (estimated size 11.5 KB, free 365.2 MB)\n",
      "2019-08-08 23:08:08 INFO  BlockManagerInfo:54 - Added broadcast_10_piece0 in memory on 192.168.1.33:36883 (size: 11.5 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:08 INFO  SparkContext:54 - Created broadcast 10 from broadcast at DAGScheduler.scala:1161\n",
      "2019-08-08 23:08:08 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 6 (PythonRDD[22] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "2019-08-08 23:08:08 INFO  TaskSchedulerImpl:54 - Adding task set 6.0 with 1 tasks\n",
      "2019-08-08 23:08:08 INFO  TaskSetManager:54 - Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 8325 bytes)\n",
      "2019-08-08 23:08:08 INFO  Executor:54 - Running task 0.0 in stage 6.0 (TID 6)\n",
      "2019-08-08 23:08:08 INFO  FileScanRDD:54 - Reading File path: file:///home/simon/PROJECTS/pybda/data/iris.tsv, range: 0-6740, partition values: [empty row]\n",
      "2019-08-08 23:08:08 INFO  PythonRunner:54 - Times: total = 55, boot = -87, init = 139, finish = 3\n",
      "2019-08-08 23:08:08 INFO  PythonRunner:54 - Times: total = 261, boot = 8, init = 55, finish = 198\n",
      "2019-08-08 23:08:08 INFO  Executor:54 - Finished task 0.0 in stage 6.0 (TID 6). 1891 bytes result sent to driver\n",
      "2019-08-08 23:08:08 INFO  TaskSetManager:54 - Finished task 0.0 in stage 6.0 (TID 6) in 278 ms on localhost (executor driver) (1/1)\n",
      "2019-08-08 23:08:08 INFO  TaskSchedulerImpl:54 - Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "2019-08-08 23:08:08 INFO  DAGScheduler:54 - ResultStage 6 (runJob at PythonRDD.scala:153) finished in 0.291 s\n",
      "2019-08-08 23:08:08 INFO  DAGScheduler:54 - Job 6 finished: runJob at PythonRDD.scala:153, took 0.294174 s\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 66\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 55\n",
      "2019-08-08 23:08:19 INFO  FileSourceStrategy:54 - Pruning directories with: \n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 185\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 156\n",
      "2019-08-08 23:08:19 INFO  FileSourceStrategy:54 - Post-Scan Filters: \n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 183\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 56\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 45\n",
      "2019-08-08 23:08:19 INFO  FileSourceStrategy:54 - Output Data Schema: struct<slength: string, swidth: string, plength: string, pwidth: string, Species: string ... 5 more fields>\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 54\n",
      "2019-08-08 23:08:19 INFO  FileSourceScanExec:54 - Pushed Filters: \n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 149\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 84\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 120\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 190\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 168\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-08 23:08:19 INFO  BlockManagerInfo:54 - Removed broadcast_5_piece0 on 192.168.1.33:36883 in memory (size: 10.7 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 88\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 109\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 176\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 90\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 167\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 52\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 75\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 102\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 170\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 123\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 142\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 128\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 48\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 61\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 72\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 153\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 127\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 145\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 53\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 46\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 187\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 139\n",
      "2019-08-08 23:08:19 INFO  BlockManagerInfo:54 - Removed broadcast_7_piece0 on 192.168.1.33:36883 in memory (size: 10.8 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 137\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 131\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 182\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 67\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 155\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 47\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 147\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 79\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 144\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 188\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 152\n",
      "2019-08-08 23:08:19 INFO  BlockManagerInfo:54 - Removed broadcast_8_piece0 on 192.168.1.33:36883 in memory (size: 10.3 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 68\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 89\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 85\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 124\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 132\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 135\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 133\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 136\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 76\n",
      "2019-08-08 23:08:19 INFO  BlockManagerInfo:54 - Removed broadcast_6_piece0 on 192.168.1.33:36883 in memory (size: 10.4 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 62\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 77\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 69\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 60\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 43\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 165\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 126\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 63\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 83\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 138\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 87\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 169\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 92\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 151\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 105\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 104\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 101\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 186\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 129\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 178\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 58\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 81\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 78\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 103\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 73\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 157\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 150\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 161\n",
      "2019-08-08 23:08:19 INFO  BlockManagerInfo:54 - Removed broadcast_10_piece0 on 192.168.1.33:36883 in memory (size: 11.5 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 121\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 95\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 143\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 98\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 107\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 146\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 86\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 82\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 49\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 74\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 110\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 163\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 51\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 162\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 181\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 106\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 113\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 154\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 159\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 125\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 148\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 100\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 114\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 91\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 134\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 173\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 172\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 175\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 160\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 171\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 112\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 44\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 164\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 93\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 94\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 122\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 141\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 130\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 99\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 174\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 59\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 115\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 117\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 177\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 50\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 191\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 71\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 184\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 189\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 42\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 116\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 166\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 57\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 158\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 180\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 179\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 70\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 119\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 97\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 65\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 64\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 80\n",
      "2019-08-08 23:08:19 INFO  BlockManagerInfo:54 - Removed broadcast_4_piece0 on 192.168.1.33:36883 in memory (size: 10.3 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:19 INFO  CodeGenerator:54 - Code generated in 7.527262 ms\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 96\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 108\n",
      "2019-08-08 23:08:19 INFO  ContextCleaner:54 - Cleaned accumulator 118\n",
      "2019-08-08 23:08:19 INFO  MemoryStore:54 - Block broadcast_11 stored as values in memory (estimated size 285.0 KB, free 365.1 MB)\n",
      "2019-08-08 23:08:19 INFO  MemoryStore:54 - Block broadcast_11_piece0 stored as bytes in memory (estimated size 23.4 KB, free 365.1 MB)\n",
      "2019-08-08 23:08:19 INFO  BlockManagerInfo:54 - Added broadcast_11_piece0 in memory on 192.168.1.33:36883 (size: 23.4 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:19 INFO  SparkContext:54 - Created broadcast 11 from take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181\n",
      "2019-08-08 23:08:19 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4201044 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2019-08-08 23:08:19 INFO  SparkContext:54 - Starting job: take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181\n",
      "2019-08-08 23:08:19 INFO  DAGScheduler:54 - Got job 7 (take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181) with 1 output partitions\n",
      "2019-08-08 23:08:19 INFO  DAGScheduler:54 - Final stage: ResultStage 7 (take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181)\n",
      "2019-08-08 23:08:19 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "2019-08-08 23:08:19 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "2019-08-08 23:08:19 INFO  DAGScheduler:54 - Submitting ResultStage 7 (MapPartitionsRDD[29] at take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181), which has no missing parents\n",
      "2019-08-08 23:08:19 INFO  MemoryStore:54 - Block broadcast_12 stored as values in memory (estimated size 11.9 KB, free 365.1 MB)\n",
      "2019-08-08 23:08:19 INFO  MemoryStore:54 - Block broadcast_12_piece0 stored as bytes in memory (estimated size 6.6 KB, free 365.1 MB)\n",
      "2019-08-08 23:08:19 INFO  BlockManagerInfo:54 - Added broadcast_12_piece0 in memory on 192.168.1.33:36883 (size: 6.6 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:19 INFO  SparkContext:54 - Created broadcast 12 from broadcast at DAGScheduler.scala:1161\n",
      "2019-08-08 23:08:19 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[29] at take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181) (first 15 tasks are for partitions Vector(0))\n",
      "2019-08-08 23:08:19 INFO  TaskSchedulerImpl:54 - Adding task set 7.0 with 1 tasks\n",
      "2019-08-08 23:08:19 INFO  TaskSetManager:54 - Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 8325 bytes)\n",
      "2019-08-08 23:08:19 INFO  Executor:54 - Running task 0.0 in stage 7.0 (TID 7)\n",
      "2019-08-08 23:08:19 INFO  FileScanRDD:54 - Reading File path: file:///home/simon/PROJECTS/pybda/data/iris.tsv, range: 0-6740, partition values: [empty row]\n",
      "2019-08-08 23:08:19 INFO  CodeGenerator:54 - Code generated in 11.657395 ms\n",
      "2019-08-08 23:08:19 INFO  Executor:54 - Finished task 0.0 in stage 7.0 (TID 7). 2789 bytes result sent to driver\n",
      "2019-08-08 23:08:19 INFO  TaskSetManager:54 - Finished task 0.0 in stage 7.0 (TID 7) in 24 ms on localhost (executor driver) (1/1)\n",
      "2019-08-08 23:08:19 INFO  TaskSchedulerImpl:54 - Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "2019-08-08 23:08:19 INFO  DAGScheduler:54 - ResultStage 7 (take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181) finished in 0.031 s\n",
      "2019-08-08 23:08:19 INFO  DAGScheduler:54 - Job 7 finished: take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181, took 0.032792 s\n",
      "2019-08-08 23:08:19 INFO  CodeGenerator:54 - Code generated in 6.715545 ms\n",
      "2019-08-08 23:08:19 INFO  MemoryStore:54 - Block broadcast_13 stored as values in memory (estimated size 1025.2 KB, free 364.1 MB)\n",
      "2019-08-08 23:08:19 INFO  MemoryStore:54 - Block broadcast_13_piece0 stored as bytes in memory (estimated size 1921.0 B, free 364.1 MB)\n",
      "2019-08-08 23:08:19 INFO  BlockManagerInfo:54 - Added broadcast_13_piece0 in memory on 192.168.1.33:36883 (size: 1921.0 B, free: 366.2 MB)\n",
      "2019-08-08 23:08:19 INFO  SparkContext:54 - Created broadcast 13 from take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181\n",
      "2019-08-08 23:08:19 INFO  CodeGenerator:54 - Code generated in 33.953553 ms\n",
      "2019-08-08 23:08:19 INFO  SparkContext:54 - Starting job: take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181\n",
      "2019-08-08 23:08:19 INFO  DAGScheduler:54 - Got job 8 (take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181) with 1 output partitions\n",
      "2019-08-08 23:08:19 INFO  DAGScheduler:54 - Final stage: ResultStage 8 (take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181)\n",
      "2019-08-08 23:08:19 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "2019-08-08 23:08:19 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "2019-08-08 23:08:19 INFO  DAGScheduler:54 - Submitting ResultStage 8 (MapPartitionsRDD[35] at take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181), which has no missing parents\n",
      "2019-08-08 23:08:19 INFO  MemoryStore:54 - Block broadcast_14 stored as values in memory (estimated size 45.4 KB, free 364.0 MB)\n",
      "2019-08-08 23:08:19 INFO  MemoryStore:54 - Block broadcast_14_piece0 stored as bytes in memory (estimated size 19.7 KB, free 364.0 MB)\n",
      "2019-08-08 23:08:19 INFO  BlockManagerInfo:54 - Added broadcast_14_piece0 in memory on 192.168.1.33:36883 (size: 19.7 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:19 INFO  SparkContext:54 - Created broadcast 14 from broadcast at DAGScheduler.scala:1161\n",
      "2019-08-08 23:08:19 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[35] at take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181) (first 15 tasks are for partitions Vector(0))\n",
      "2019-08-08 23:08:19 INFO  TaskSchedulerImpl:54 - Adding task set 8.0 with 1 tasks\n",
      "2019-08-08 23:08:19 INFO  TaskSetManager:54 - Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 8325 bytes)\n",
      "2019-08-08 23:08:19 INFO  Executor:54 - Running task 0.0 in stage 8.0 (TID 8)\n",
      "2019-08-08 23:08:19 INFO  FileScanRDD:54 - Reading File path: file:///home/simon/PROJECTS/pybda/data/iris.tsv, range: 0-6740, partition values: [empty row]\n",
      "2019-08-08 23:08:19 INFO  CodeGenerator:54 - Code generated in 11.426173 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-08 23:08:19 INFO  CodeGenerator:54 - Code generated in 5.887212 ms\n",
      "2019-08-08 23:08:19 INFO  PythonRunner:54 - Times: total = 45, boot = -10859, init = 10901, finish = 3\n",
      "2019-08-08 23:08:19 INFO  CodeGenerator:54 - Code generated in 22.57845 ms\n",
      "2019-08-08 23:08:19 INFO  PythonRunner:54 - Times: total = 241, boot = 3, init = 237, finish = 1\n",
      "2019-08-08 23:08:19 INFO  Executor:54 - Finished task 0.0 in stage 8.0 (TID 8). 2250 bytes result sent to driver\n",
      "2019-08-08 23:08:19 INFO  TaskSetManager:54 - Finished task 0.0 in stage 8.0 (TID 8) in 466 ms on localhost (executor driver) (1/1)\n",
      "2019-08-08 23:08:19 INFO  TaskSchedulerImpl:54 - Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "2019-08-08 23:08:19 INFO  DAGScheduler:54 - ResultStage 8 (take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181) finished in 0.474 s\n",
      "2019-08-08 23:08:19 INFO  DAGScheduler:54 - Job 8 finished: take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181, took 0.476611 s\n",
      "2019-08-08 23:08:20 INFO  FileSourceStrategy:54 - Pruning directories with: \n",
      "2019-08-08 23:08:20 INFO  FileSourceStrategy:54 - Post-Scan Filters: \n",
      "2019-08-08 23:08:20 INFO  FileSourceStrategy:54 - Output Data Schema: struct<slength: string, swidth: string, plength: string, pwidth: string, Species: string ... 5 more fields>\n",
      "2019-08-08 23:08:20 INFO  FileSourceScanExec:54 - Pushed Filters: \n",
      "2019-08-08 23:08:20 INFO  CodeGenerator:54 - Code generated in 14.916485 ms\n",
      "2019-08-08 23:08:20 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1\n",
      "2019-08-08 23:08:20 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "2019-08-08 23:08:20 INFO  MemoryStore:54 - Block broadcast_15 stored as values in memory (estimated size 285.0 KB, free 363.7 MB)\n",
      "2019-08-08 23:08:20 INFO  MemoryStore:54 - Block broadcast_15_piece0 stored as bytes in memory (estimated size 23.4 KB, free 363.7 MB)\n",
      "2019-08-08 23:08:20 INFO  BlockManagerInfo:54 - Added broadcast_15_piece0 in memory on 192.168.1.33:36883 (size: 23.4 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:20 INFO  SparkContext:54 - Created broadcast 15 from run at ThreadPoolExecutor.java:1149\n",
      "2019-08-08 23:08:20 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4201044 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2019-08-08 23:08:20 INFO  SparkContext:54 - Starting job: run at ThreadPoolExecutor.java:1149\n",
      "2019-08-08 23:08:20 INFO  DAGScheduler:54 - Got job 9 (run at ThreadPoolExecutor.java:1149) with 1 output partitions\n",
      "2019-08-08 23:08:20 INFO  DAGScheduler:54 - Final stage: ResultStage 9 (run at ThreadPoolExecutor.java:1149)\n",
      "2019-08-08 23:08:20 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "2019-08-08 23:08:20 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "2019-08-08 23:08:20 INFO  DAGScheduler:54 - Submitting ResultStage 9 (MapPartitionsRDD[38] at run at ThreadPoolExecutor.java:1149), which has no missing parents\n",
      "2019-08-08 23:08:20 INFO  MemoryStore:54 - Block broadcast_16 stored as values in memory (estimated size 16.8 KB, free 363.7 MB)\n",
      "2019-08-08 23:08:20 INFO  MemoryStore:54 - Block broadcast_16_piece0 stored as bytes in memory (estimated size 8.0 KB, free 363.7 MB)\n",
      "2019-08-08 23:08:20 INFO  BlockManagerInfo:54 - Added broadcast_16_piece0 in memory on 192.168.1.33:36883 (size: 8.0 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:20 INFO  SparkContext:54 - Created broadcast 16 from broadcast at DAGScheduler.scala:1161\n",
      "2019-08-08 23:08:20 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[38] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))\n",
      "2019-08-08 23:08:20 INFO  TaskSchedulerImpl:54 - Adding task set 9.0 with 1 tasks\n",
      "2019-08-08 23:08:20 INFO  TaskSetManager:54 - Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 8325 bytes)\n",
      "2019-08-08 23:08:20 INFO  CodeGenerator:54 - Code generated in 10.578167 ms\n",
      "2019-08-08 23:08:20 INFO  Executor:54 - Running task 0.0 in stage 9.0 (TID 9)\n",
      "2019-08-08 23:08:20 INFO  FileScanRDD:54 - Reading File path: file:///home/simon/PROJECTS/pybda/data/iris.tsv, range: 0-6740, partition values: [empty row]\n",
      "2019-08-08 23:08:20 INFO  Executor:54 - Finished task 0.0 in stage 9.0 (TID 9). 8070 bytes result sent to driver\n",
      "2019-08-08 23:08:20 INFO  TaskSetManager:54 - Finished task 0.0 in stage 9.0 (TID 9) in 12 ms on localhost (executor driver) (1/1)\n",
      "2019-08-08 23:08:20 INFO  TaskSchedulerImpl:54 - Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "2019-08-08 23:08:20 INFO  DAGScheduler:54 - ResultStage 9 (run at ThreadPoolExecutor.java:1149) finished in 0.018 s\n",
      "2019-08-08 23:08:20 INFO  DAGScheduler:54 - Job 9 finished: run at ThreadPoolExecutor.java:1149, took 0.021132 s\n",
      "2019-08-08 23:08:20 INFO  CodeGenerator:54 - Code generated in 5.029258 ms\n",
      "2019-08-08 23:08:20 INFO  MemoryStore:54 - Block broadcast_17 stored as values in memory (estimated size 1025.2 KB, free 362.7 MB)\n",
      "2019-08-08 23:08:20 INFO  MemoryStore:54 - Block broadcast_17_piece0 stored as bytes in memory (estimated size 7.4 KB, free 362.7 MB)\n",
      "2019-08-08 23:08:20 INFO  BlockManagerInfo:54 - Added broadcast_17_piece0 in memory on 192.168.1.33:36883 (size: 7.4 KB, free: 366.1 MB)\n",
      "2019-08-08 23:08:20 INFO  SparkContext:54 - Created broadcast 17 from run at ThreadPoolExecutor.java:1149\n",
      "2019-08-08 23:08:20 INFO  CodeGenerator:54 - Code generated in 25.060953 ms\n",
      "2019-08-08 23:08:20 INFO  SparkContext:54 - Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "2019-08-08 23:08:20 INFO  DAGScheduler:54 - Got job 10 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2019-08-08 23:08:20 INFO  DAGScheduler:54 - Final stage: ResultStage 10 (csv at NativeMethodAccessorImpl.java:0)\n",
      "2019-08-08 23:08:20 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "2019-08-08 23:08:20 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "2019-08-08 23:08:20 INFO  DAGScheduler:54 - Submitting ResultStage 10 (CoalescedRDD[53] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2019-08-08 23:08:20 INFO  MemoryStore:54 - Block broadcast_18 stored as values in memory (estimated size 196.9 KB, free 362.5 MB)\n",
      "2019-08-08 23:08:20 INFO  MemoryStore:54 - Block broadcast_18_piece0 stored as bytes in memory (estimated size 72.9 KB, free 362.4 MB)\n",
      "2019-08-08 23:08:20 INFO  BlockManagerInfo:54 - Added broadcast_18_piece0 in memory on 192.168.1.33:36883 (size: 72.9 KB, free: 366.1 MB)\n",
      "2019-08-08 23:08:20 INFO  SparkContext:54 - Created broadcast 18 from broadcast at DAGScheduler.scala:1161\n",
      "2019-08-08 23:08:20 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 10 (CoalescedRDD[53] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2019-08-08 23:08:20 INFO  TaskSchedulerImpl:54 - Adding task set 10.0 with 1 tasks\n",
      "2019-08-08 23:08:20 INFO  TaskSetManager:54 - Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 8534 bytes)\n",
      "2019-08-08 23:08:20 INFO  Executor:54 - Running task 0.0 in stage 10.0 (TID 10)\n",
      "2019-08-08 23:08:20 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1\n",
      "2019-08-08 23:08:20 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "2019-08-08 23:08:20 INFO  FileScanRDD:54 - Reading File path: file:///home/simon/PROJECTS/pybda/data/iris.tsv, range: 0-6740, partition values: [empty row]\n",
      "2019-08-08 23:08:20 INFO  CodeGenerator:54 - Code generated in 5.298743 ms\n",
      "2019-08-08 23:08:20 INFO  PythonRunner:54 - Times: total = 50, boot = -989, init = 1036, finish = 3\n",
      "2019-08-08 23:08:20 INFO  CodeGenerator:54 - Code generated in 24.015372 ms\n",
      "2019-08-08 23:08:20 INFO  PythonRunner:54 - Times: total = 52, boot = -802, init = 852, finish = 2\n",
      "2019-08-08 23:08:20 INFO  PythonUDFRunner:54 - Times: total = 249, boot = 3, init = 227, finish = 19\n",
      "2019-08-08 23:08:20 INFO  PythonUDFRunner:54 - Times: total = 409, boot = 5, init = 238, finish = 166\n",
      "2019-08-08 23:08:20 INFO  FileOutputCommitter:535 - Saved output of task 'attempt_20190808230820_0010_m_000000_0' to file:/home/simon/PROJECTS/pybda/data/results/2019_08_08/pca_from_iris/_temporary/0/task_20190808230820_0010_m_000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-08 23:08:20 INFO  SparkHadoopMapRedUtil:54 - attempt_20190808230820_0010_m_000000_0: Committed\n",
      "2019-08-08 23:08:20 INFO  Executor:54 - Finished task 0.0 in stage 10.0 (TID 10). 4019 bytes result sent to driver\n",
      "2019-08-08 23:08:20 INFO  TaskSetManager:54 - Finished task 0.0 in stage 10.0 (TID 10) in 552 ms on localhost (executor driver) (1/1)\n",
      "2019-08-08 23:08:20 INFO  TaskSchedulerImpl:54 - Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "2019-08-08 23:08:20 INFO  DAGScheduler:54 - ResultStage 10 (csv at NativeMethodAccessorImpl.java:0) finished in 0.573 s\n",
      "2019-08-08 23:08:20 INFO  DAGScheduler:54 - Job 10 finished: csv at NativeMethodAccessorImpl.java:0, took 0.578634 s\n",
      "2019-08-08 23:08:20 INFO  FileFormatWriter:54 - Write Job 60e5f028-c6ae-4bb0-932e-6391807683b7 committed.\n",
      "2019-08-08 23:08:20 INFO  FileFormatWriter:54 - Finished processing stats for write job 60e5f028-c6ae-4bb0-932e-6391807683b7.\n",
      "2019-08-08 23:08:21 INFO  FileSourceStrategy:54 - Pruning directories with: \n",
      "2019-08-08 23:08:21 INFO  FileSourceStrategy:54 - Post-Scan Filters: \n",
      "2019-08-08 23:08:21 INFO  FileSourceStrategy:54 - Output Data Schema: struct<slength: string, swidth: string, plength: string, pwidth: string, Species: string ... 5 more fields>\n",
      "2019-08-08 23:08:21 INFO  FileSourceScanExec:54 - Pushed Filters: \n",
      "2019-08-08 23:08:21 INFO  MemoryStore:54 - Block broadcast_19 stored as values in memory (estimated size 285.0 KB, free 362.1 MB)\n",
      "2019-08-08 23:08:21 INFO  CodeGenerator:54 - Code generated in 9.018456 ms\n",
      "2019-08-08 23:08:21 INFO  MemoryStore:54 - Block broadcast_19_piece0 stored as bytes in memory (estimated size 23.4 KB, free 362.1 MB)\n",
      "2019-08-08 23:08:21 INFO  BlockManagerInfo:54 - Added broadcast_19_piece0 in memory on 192.168.1.33:36883 (size: 23.4 KB, free: 366.0 MB)\n",
      "2019-08-08 23:08:21 INFO  SparkContext:54 - Created broadcast 19 from run at ThreadPoolExecutor.java:1149\n",
      "2019-08-08 23:08:21 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4201044 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2019-08-08 23:08:21 INFO  SparkContext:54 - Starting job: run at ThreadPoolExecutor.java:1149\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Got job 11 (run at ThreadPoolExecutor.java:1149) with 1 output partitions\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Final stage: ResultStage 11 (run at ThreadPoolExecutor.java:1149)\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Submitting ResultStage 11 (MapPartitionsRDD[57] at run at ThreadPoolExecutor.java:1149), which has no missing parents\n",
      "2019-08-08 23:08:21 INFO  MemoryStore:54 - Block broadcast_20 stored as values in memory (estimated size 11.9 KB, free 362.1 MB)\n",
      "2019-08-08 23:08:21 INFO  MemoryStore:54 - Block broadcast_20_piece0 stored as bytes in memory (estimated size 6.6 KB, free 362.1 MB)\n",
      "2019-08-08 23:08:21 INFO  BlockManagerInfo:54 - Added broadcast_20_piece0 in memory on 192.168.1.33:36883 (size: 6.6 KB, free: 366.0 MB)\n",
      "2019-08-08 23:08:21 INFO  SparkContext:54 - Created broadcast 20 from broadcast at DAGScheduler.scala:1161\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[57] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))\n",
      "2019-08-08 23:08:21 INFO  TaskSchedulerImpl:54 - Adding task set 11.0 with 1 tasks\n",
      "2019-08-08 23:08:21 INFO  TaskSetManager:54 - Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 8325 bytes)\n",
      "2019-08-08 23:08:21 INFO  Executor:54 - Running task 0.0 in stage 11.0 (TID 11)\n",
      "2019-08-08 23:08:21 INFO  FileScanRDD:54 - Reading File path: file:///home/simon/PROJECTS/pybda/data/iris.tsv, range: 0-6740, partition values: [empty row]\n",
      "2019-08-08 23:08:21 INFO  Executor:54 - Finished task 0.0 in stage 11.0 (TID 11). 2789 bytes result sent to driver\n",
      "2019-08-08 23:08:21 INFO  TaskSetManager:54 - Finished task 0.0 in stage 11.0 (TID 11) in 10 ms on localhost (executor driver) (1/1)\n",
      "2019-08-08 23:08:21 INFO  TaskSchedulerImpl:54 - Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - ResultStage 11 (run at ThreadPoolExecutor.java:1149) finished in 0.018 s\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Job 11 finished: run at ThreadPoolExecutor.java:1149, took 0.019846 s\n",
      "2019-08-08 23:08:21 INFO  MemoryStore:54 - Block broadcast_21 stored as values in memory (estimated size 1025.2 KB, free 361.1 MB)\n",
      "2019-08-08 23:08:21 INFO  MemoryStore:54 - Block broadcast_21_piece0 stored as bytes in memory (estimated size 1921.0 B, free 361.1 MB)\n",
      "2019-08-08 23:08:21 INFO  BlockManagerInfo:54 - Added broadcast_21_piece0 in memory on 192.168.1.33:36883 (size: 1921.0 B, free: 366.0 MB)\n",
      "2019-08-08 23:08:21 INFO  SparkContext:54 - Created broadcast 21 from run at ThreadPoolExecutor.java:1149\n",
      "2019-08-08 23:08:21 INFO  CodeGenerator:54 - Code generated in 12.049541 ms\n",
      "2019-08-08 23:08:21 INFO  SparkContext:54 - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Registering RDD 60 (count at NativeMethodAccessorImpl.java:0)\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Got job 12 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Final stage: ResultStage 13 (count at NativeMethodAccessorImpl.java:0)\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 12)\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 12)\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 12 (MapPartitionsRDD[60] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2019-08-08 23:08:21 INFO  MemoryStore:54 - Block broadcast_22 stored as values in memory (estimated size 33.4 KB, free 361.1 MB)\n",
      "2019-08-08 23:08:21 INFO  MemoryStore:54 - Block broadcast_22_piece0 stored as bytes in memory (estimated size 16.7 KB, free 361.0 MB)\n",
      "2019-08-08 23:08:21 INFO  BlockManagerInfo:54 - Added broadcast_22_piece0 in memory on 192.168.1.33:36883 (size: 16.7 KB, free: 366.0 MB)\n",
      "2019-08-08 23:08:21 INFO  SparkContext:54 - Created broadcast 22 from broadcast at DAGScheduler.scala:1161\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[60] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2019-08-08 23:08:21 INFO  TaskSchedulerImpl:54 - Adding task set 12.0 with 1 tasks\n",
      "2019-08-08 23:08:21 INFO  TaskSetManager:54 - Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 8314 bytes)\n",
      "2019-08-08 23:08:21 INFO  Executor:54 - Running task 0.0 in stage 12.0 (TID 12)\n",
      "2019-08-08 23:08:21 INFO  FileScanRDD:54 - Reading File path: file:///home/simon/PROJECTS/pybda/data/iris.tsv, range: 0-6740, partition values: [empty row]\n",
      "2019-08-08 23:08:21 INFO  PythonRunner:54 - Times: total = 45, boot = -708, init = 750, finish = 3\n",
      "2019-08-08 23:08:21 INFO  PythonRunner:54 - Times: total = 56, boot = -704, init = 758, finish = 2\n",
      "2019-08-08 23:08:21 INFO  Executor:54 - Finished task 0.0 in stage 12.0 (TID 12). 2645 bytes result sent to driver\n",
      "2019-08-08 23:08:21 INFO  TaskSetManager:54 - Finished task 0.0 in stage 12.0 (TID 12) in 89 ms on localhost (executor driver) (1/1)\n",
      "2019-08-08 23:08:21 INFO  TaskSchedulerImpl:54 - Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - ShuffleMapStage 12 (count at NativeMethodAccessorImpl.java:0) finished in 0.101 s\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - looking for newly runnable stages\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - running: Set()\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - waiting: Set(ResultStage 13)\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - failed: Set()\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Submitting ResultStage 13 (MapPartitionsRDD[63] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2019-08-08 23:08:21 INFO  MemoryStore:54 - Block broadcast_23 stored as values in memory (estimated size 7.1 KB, free 361.0 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-08 23:08:21 INFO  MemoryStore:54 - Block broadcast_23_piece0 stored as bytes in memory (estimated size 3.8 KB, free 361.0 MB)\n",
      "2019-08-08 23:08:21 INFO  BlockManagerInfo:54 - Added broadcast_23_piece0 in memory on 192.168.1.33:36883 (size: 3.8 KB, free: 366.0 MB)\n",
      "2019-08-08 23:08:21 INFO  SparkContext:54 - Created broadcast 23 from broadcast at DAGScheduler.scala:1161\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[63] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2019-08-08 23:08:21 INFO  TaskSchedulerImpl:54 - Adding task set 13.0 with 1 tasks\n",
      "2019-08-08 23:08:21 INFO  TaskSetManager:54 - Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, ANY, 7767 bytes)\n",
      "2019-08-08 23:08:21 INFO  Executor:54 - Running task 0.0 in stage 13.0 (TID 13)\n",
      "2019-08-08 23:08:21 INFO  ShuffleBlockFetcherIterator:54 - Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\n",
      "2019-08-08 23:08:21 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 5 ms\n",
      "2019-08-08 23:08:21 INFO  Executor:54 - Finished task 0.0 in stage 13.0 (TID 13). 1739 bytes result sent to driver\n",
      "2019-08-08 23:08:21 INFO  TaskSetManager:54 - Finished task 0.0 in stage 13.0 (TID 13) in 38 ms on localhost (executor driver) (1/1)\n",
      "2019-08-08 23:08:21 INFO  TaskSchedulerImpl:54 - Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - ResultStage 13 (count at NativeMethodAccessorImpl.java:0) finished in 0.047 s\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Job 12 finished: count at NativeMethodAccessorImpl.java:0, took 0.168176 s\n",
      "2019-08-08 23:08:21 INFO  BlockManagerInfo:54 - Removed broadcast_21_piece0 on 192.168.1.33:36883 in memory (size: 1921.0 B, free: 366.0 MB)\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 398\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 363\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 371\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 325\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 322\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 415\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 316\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 400\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 352\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 373\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 317\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 361\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 341\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 388\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 327\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 298\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 408\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 368\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 308\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 350\n",
      "2019-08-08 23:08:21 INFO  BlockManagerInfo:54 - Removed broadcast_22_piece0 on 192.168.1.33:36883 in memory (size: 16.7 KB, free: 366.0 MB)\n",
      "2019-08-08 23:08:21 INFO  FileSourceStrategy:54 - Pruning directories with: \n",
      "2019-08-08 23:08:21 INFO  FileSourceStrategy:54 - Post-Scan Filters: \n",
      "2019-08-08 23:08:21 INFO  FileSourceStrategy:54 - Output Data Schema: struct<slength: string, swidth: string, plength: string, pwidth: string, Species: string ... 5 more fields>\n",
      "2019-08-08 23:08:21 INFO  FileSourceScanExec:54 - Pushed Filters: \n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 328\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 314\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 406\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 380\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 394\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 299\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 399\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 307\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 318\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 303\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned shuffle 0\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 365\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 347\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 353\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 349\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 381\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 369\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 329\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 330\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 358\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 419\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 339\n",
      "2019-08-08 23:08:21 INFO  BlockManagerInfo:54 - Removed broadcast_23_piece0 on 192.168.1.33:36883 in memory (size: 3.8 KB, free: 366.0 MB)\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 393\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 390\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 422\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 332\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 416\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 359\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 377\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 312\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 412\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 315\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 321\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 326\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 302\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 300\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 345\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 407\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 305\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 356\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 331\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 372\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 333\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 411\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 346\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 336\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 387\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 343\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 421\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 320\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 374\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 364\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 376\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 391\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 414\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 413\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 306\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 357\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 366\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 403\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 379\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 409\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 418\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 402\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 392\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 410\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 335\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 386\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 310\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 420\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 351\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 338\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 323\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 389\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 384\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 311\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 342\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 362\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 319\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 385\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 309\n",
      "2019-08-08 23:08:21 INFO  BlockManagerInfo:54 - Removed broadcast_18_piece0 on 192.168.1.33:36883 in memory (size: 72.9 KB, free: 366.1 MB)\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 405\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 397\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 301\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 313\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 355\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 396\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 367\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 324\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 360\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 423\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 395\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 337\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 375\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 304\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 417\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 354\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 382\n",
      "2019-08-08 23:08:21 INFO  BlockManagerInfo:54 - Removed broadcast_19_piece0 on 192.168.1.33:36883 in memory (size: 23.4 KB, free: 366.1 MB)\n",
      "2019-08-08 23:08:21 INFO  MemoryStore:54 - Block broadcast_24 stored as values in memory (estimated size 285.0 KB, free 362.4 MB)\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 383\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 370\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 404\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 348\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 340\n",
      "2019-08-08 23:08:21 INFO  MemoryStore:54 - Block broadcast_24_piece0 stored as bytes in memory (estimated size 23.4 KB, free 362.4 MB)\n",
      "2019-08-08 23:08:21 INFO  BlockManagerInfo:54 - Removed broadcast_20_piece0 on 192.168.1.33:36883 in memory (size: 6.6 KB, free: 366.1 MB)\n",
      "2019-08-08 23:08:21 INFO  BlockManagerInfo:54 - Added broadcast_24_piece0 in memory on 192.168.1.33:36883 (size: 23.4 KB, free: 366.1 MB)\n",
      "2019-08-08 23:08:21 INFO  SparkContext:54 - Created broadcast 24 from take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181\n",
      "2019-08-08 23:08:21 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4201044 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 344\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 401\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 378\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 236\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 282\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 218\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 206\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 40\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 258\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 268\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 234\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 292\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 256\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 244\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 246\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 247\n",
      "2019-08-08 23:08:21 INFO  BlockManagerInfo:54 - Removed broadcast_17_piece0 on 192.168.1.33:36883 in memory (size: 7.4 KB, free: 366.1 MB)\n",
      "2019-08-08 23:08:21 INFO  SparkContext:54 - Starting job: take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Got job 13 (take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181) with 1 output partitions\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Final stage: ResultStage 14 (take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181)\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Submitting ResultStage 14 (MapPartitionsRDD[66] at take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181), which has no missing parents\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 260\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 245\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 289\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 239\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 264\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 276\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 209\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 296\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 2\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 295\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 205\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 297\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 207\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 222\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 288\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 294\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 215\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 275\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 231\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 224\n",
      "2019-08-08 23:08:21 INFO  MemoryStore:54 - Block broadcast_25 stored as values in memory (estimated size 11.9 KB, free 363.4 MB)\n",
      "2019-08-08 23:08:21 INFO  MemoryStore:54 - Block broadcast_25_piece0 stored as bytes in memory (estimated size 6.6 KB, free 363.4 MB)\n",
      "2019-08-08 23:08:21 INFO  BlockManagerInfo:54 - Added broadcast_25_piece0 in memory on 192.168.1.33:36883 (size: 6.6 KB, free: 366.1 MB)\n",
      "2019-08-08 23:08:21 INFO  SparkContext:54 - Created broadcast 25 from broadcast at DAGScheduler.scala:1161\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[66] at take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181) (first 15 tasks are for partitions Vector(0))\n",
      "2019-08-08 23:08:21 INFO  TaskSchedulerImpl:54 - Adding task set 14.0 with 1 tasks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-08 23:08:21 INFO  BlockManagerInfo:54 - Removed broadcast_2_piece0 on 192.168.1.33:36883 in memory (size: 23.4 KB, free: 366.1 MB)\n",
      "2019-08-08 23:08:21 INFO  TaskSetManager:54 - Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, PROCESS_LOCAL, 8325 bytes)\n",
      "2019-08-08 23:08:21 INFO  Executor:54 - Running task 0.0 in stage 14.0 (TID 14)\n",
      "2019-08-08 23:08:21 INFO  FileScanRDD:54 - Reading File path: file:///home/simon/PROJECTS/pybda/data/iris.tsv, range: 0-6740, partition values: [empty row]\n",
      "2019-08-08 23:08:21 INFO  Executor:54 - Finished task 0.0 in stage 14.0 (TID 14). 2789 bytes result sent to driver\n",
      "2019-08-08 23:08:21 INFO  TaskSetManager:54 - Finished task 0.0 in stage 14.0 (TID 14) in 14 ms on localhost (executor driver) (1/1)\n",
      "2019-08-08 23:08:21 INFO  TaskSchedulerImpl:54 - Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - ResultStage 14 (take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181) finished in 0.022 s\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Job 13 finished: take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181, took 0.023812 s\n",
      "2019-08-08 23:08:21 INFO  MemoryStore:54 - Block broadcast_26 stored as values in memory (estimated size 1025.2 KB, free 362.7 MB)\n",
      "2019-08-08 23:08:21 INFO  MemoryStore:54 - Block broadcast_26_piece0 stored as bytes in memory (estimated size 1921.0 B, free 362.7 MB)\n",
      "2019-08-08 23:08:21 INFO  BlockManagerInfo:54 - Added broadcast_26_piece0 in memory on 192.168.1.33:36883 (size: 1921.0 B, free: 366.1 MB)\n",
      "2019-08-08 23:08:21 INFO  SparkContext:54 - Created broadcast 26 from take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181\n",
      "2019-08-08 23:08:21 INFO  BlockManagerInfo:54 - Removed broadcast_15_piece0 on 192.168.1.33:36883 in memory (size: 23.4 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:21 INFO  CodeGenerator:54 - Code generated in 31.971962 ms\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 259\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 220\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 270\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 34\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 237\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 251\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 39\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 284\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 280\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 211\n",
      "2019-08-08 23:08:21 INFO  BlockManagerInfo:54 - Removed broadcast_16_piece0 on 192.168.1.33:36883 in memory (size: 8.0 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 36\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 266\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 242\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 250\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 216\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 240\n",
      "2019-08-08 23:08:21 INFO  BlockManagerInfo:54 - Removed broadcast_0_piece0 on 192.168.1.33:36883 in memory (size: 23.4 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 238\n",
      "2019-08-08 23:08:21 INFO  BlockManagerInfo:54 - Removed broadcast_12_piece0 on 192.168.1.33:36883 in memory (size: 6.6 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 35\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 1\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 212\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 286\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 235\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 213\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 33\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 290\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 252\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 243\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 219\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 210\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 283\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 226\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 32\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 261\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 232\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 279\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 269\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 217\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 291\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 248\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 249\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 5\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 225\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 223\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 281\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 233\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 230\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 267\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 265\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 229\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 262\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 4\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 272\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 241\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 6\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 253\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 277\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 38\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 293\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 255\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 271\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 257\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 274\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 3\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 254\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 208\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 285\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 227\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 278\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 228\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 273\n",
      "2019-08-08 23:08:21 INFO  BlockManagerInfo:54 - Removed broadcast_14_piece0 on 192.168.1.33:36883 in memory (size: 19.7 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:21 INFO  SparkContext:54 - Starting job: take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Got job 14 (take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181) with 1 output partitions\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Final stage: ResultStage 15 (take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181)\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Submitting ResultStage 15 (MapPartitionsRDD[72] at take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181), which has no missing parents\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 263\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 214\n",
      "2019-08-08 23:08:21 INFO  ContextCleaner:54 - Cleaned accumulator 221\n",
      "2019-08-08 23:08:21 INFO  MemoryStore:54 - Block broadcast_27 stored as values in memory (estimated size 48.6 KB, free 363.3 MB)\n",
      "2019-08-08 23:08:21 INFO  MemoryStore:54 - Block broadcast_27_piece0 stored as bytes in memory (estimated size 20.4 KB, free 363.3 MB)\n",
      "2019-08-08 23:08:21 INFO  BlockManagerInfo:54 - Added broadcast_27_piece0 in memory on 192.168.1.33:36883 (size: 20.4 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:21 INFO  SparkContext:54 - Created broadcast 27 from broadcast at DAGScheduler.scala:1161\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[72] at take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181) (first 15 tasks are for partitions Vector(0))\n",
      "2019-08-08 23:08:21 INFO  TaskSchedulerImpl:54 - Adding task set 15.0 with 1 tasks\n",
      "2019-08-08 23:08:21 INFO  TaskSetManager:54 - Starting task 0.0 in stage 15.0 (TID 15, localhost, executor driver, partition 0, PROCESS_LOCAL, 8325 bytes)\n",
      "2019-08-08 23:08:21 INFO  Executor:54 - Running task 0.0 in stage 15.0 (TID 15)\n",
      "2019-08-08 23:08:21 INFO  FileScanRDD:54 - Reading File path: file:///home/simon/PROJECTS/pybda/data/iris.tsv, range: 0-6740, partition values: [empty row]\n",
      "2019-08-08 23:08:21 INFO  PythonRunner:54 - Times: total = 13, boot = -988, init = 998, finish = 3\n",
      "2019-08-08 23:08:21 INFO  PythonRunner:54 - Times: total = 30, boot = -808, init = 837, finish = 1\n",
      "2019-08-08 23:08:21 INFO  Executor:54 - Finished task 0.0 in stage 15.0 (TID 15). 2312 bytes result sent to driver\n",
      "2019-08-08 23:08:21 INFO  TaskSetManager:54 - Finished task 0.0 in stage 15.0 (TID 15) in 68 ms on localhost (executor driver) (1/1)\n",
      "2019-08-08 23:08:21 INFO  TaskSchedulerImpl:54 - Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - ResultStage 15 (take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181) finished in 0.074 s\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Job 14 finished: take at /home/simon/PROJECTS/pybda/pybda/spark/features.py:181, took 0.085698 s\n",
      "2019-08-08 23:08:21 INFO  FileSourceStrategy:54 - Pruning directories with: \n",
      "2019-08-08 23:08:21 INFO  FileSourceStrategy:54 - Post-Scan Filters: \n",
      "2019-08-08 23:08:21 INFO  FileSourceStrategy:54 - Output Data Schema: struct<slength: string, swidth: string, plength: string, pwidth: string, Species: string ... 5 more fields>\n",
      "2019-08-08 23:08:21 INFO  FileSourceScanExec:54 - Pushed Filters: \n",
      "2019-08-08 23:08:21 INFO  CodeGenerator:54 - Code generated in 8.024808 ms\n",
      "2019-08-08 23:08:21 INFO  MemoryStore:54 - Block broadcast_28 stored as values in memory (estimated size 285.0 KB, free 363.0 MB)\n",
      "2019-08-08 23:08:21 INFO  MemoryStore:54 - Block broadcast_28_piece0 stored as bytes in memory (estimated size 23.4 KB, free 363.0 MB)\n",
      "2019-08-08 23:08:21 INFO  BlockManagerInfo:54 - Added broadcast_28_piece0 in memory on 192.168.1.33:36883 (size: 23.4 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:21 INFO  SparkContext:54 - Created broadcast 28 from toPandas at /home/simon/PROJECTS/pybda/pybda/util/cast_as.py:50\n",
      "2019-08-08 23:08:21 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4201044 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2019-08-08 23:08:21 INFO  SparkContext:54 - Starting job: toPandas at /home/simon/PROJECTS/pybda/pybda/util/cast_as.py:50\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Got job 15 (toPandas at /home/simon/PROJECTS/pybda/pybda/util/cast_as.py:50) with 1 output partitions\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Final stage: ResultStage 16 (toPandas at /home/simon/PROJECTS/pybda/pybda/util/cast_as.py:50)\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Submitting ResultStage 16 (MapPartitionsRDD[75] at toPandas at /home/simon/PROJECTS/pybda/pybda/util/cast_as.py:50), which has no missing parents\n",
      "2019-08-08 23:08:21 INFO  MemoryStore:54 - Block broadcast_29 stored as values in memory (estimated size 16.8 KB, free 363.0 MB)\n",
      "2019-08-08 23:08:21 INFO  MemoryStore:54 - Block broadcast_29_piece0 stored as bytes in memory (estimated size 8.1 KB, free 363.0 MB)\n",
      "2019-08-08 23:08:21 INFO  BlockManagerInfo:54 - Added broadcast_29_piece0 in memory on 192.168.1.33:36883 (size: 8.1 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:21 INFO  SparkContext:54 - Created broadcast 29 from broadcast at DAGScheduler.scala:1161\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[75] at toPandas at /home/simon/PROJECTS/pybda/pybda/util/cast_as.py:50) (first 15 tasks are for partitions Vector(0))\n",
      "2019-08-08 23:08:21 INFO  TaskSchedulerImpl:54 - Adding task set 16.0 with 1 tasks\n",
      "2019-08-08 23:08:21 INFO  TaskSetManager:54 - Starting task 0.0 in stage 16.0 (TID 16, localhost, executor driver, partition 0, PROCESS_LOCAL, 8325 bytes)\n",
      "2019-08-08 23:08:21 INFO  Executor:54 - Running task 0.0 in stage 16.0 (TID 16)\n",
      "2019-08-08 23:08:21 INFO  FileScanRDD:54 - Reading File path: file:///home/simon/PROJECTS/pybda/data/iris.tsv, range: 0-6740, partition values: [empty row]\n",
      "2019-08-08 23:08:21 INFO  Executor:54 - Finished task 0.0 in stage 16.0 (TID 16). 8070 bytes result sent to driver\n",
      "2019-08-08 23:08:21 INFO  TaskSetManager:54 - Finished task 0.0 in stage 16.0 (TID 16) in 10 ms on localhost (executor driver) (1/1)\n",
      "2019-08-08 23:08:21 INFO  TaskSchedulerImpl:54 - Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - ResultStage 16 (toPandas at /home/simon/PROJECTS/pybda/pybda/util/cast_as.py:50) finished in 0.015 s\n",
      "2019-08-08 23:08:21 INFO  DAGScheduler:54 - Job 15 finished: toPandas at /home/simon/PROJECTS/pybda/pybda/util/cast_as.py:50, took 0.017331 s\n",
      "2019-08-08 23:08:21 INFO  MemoryStore:54 - Block broadcast_30 stored as values in memory (estimated size 1025.2 KB, free 362.0 MB)\n",
      "2019-08-08 23:08:21 INFO  MemoryStore:54 - Block broadcast_30_piece0 stored as bytes in memory (estimated size 7.4 KB, free 362.0 MB)\n",
      "2019-08-08 23:08:21 INFO  BlockManagerInfo:54 - Added broadcast_30_piece0 in memory on 192.168.1.33:36883 (size: 7.4 KB, free: 366.2 MB)\n",
      "2019-08-08 23:08:21 INFO  SparkContext:54 - Created broadcast 30 from toPandas at /home/simon/PROJECTS/pybda/pybda/util/cast_as.py:50\n",
      "2019-08-08 23:08:22 INFO  CodeGenerator:54 - Code generated in 21.298122 ms\n",
      "2019-08-08 23:08:22 INFO  SparkContext:54 - Starting job: toPandas at /home/simon/PROJECTS/pybda/pybda/util/cast_as.py:50\n",
      "2019-08-08 23:08:22 INFO  DAGScheduler:54 - Got job 16 (toPandas at /home/simon/PROJECTS/pybda/pybda/util/cast_as.py:50) with 1 output partitions\n",
      "2019-08-08 23:08:22 INFO  DAGScheduler:54 - Final stage: ResultStage 17 (toPandas at /home/simon/PROJECTS/pybda/pybda/util/cast_as.py:50)\n",
      "2019-08-08 23:08:22 INFO  DAGScheduler:54 - Parents of final stage: List()\n",
      "2019-08-08 23:08:22 INFO  DAGScheduler:54 - Missing parents: List()\n",
      "2019-08-08 23:08:22 INFO  DAGScheduler:54 - Submitting ResultStage 17 (MapPartitionsRDD[83] at toPandas at /home/simon/PROJECTS/pybda/pybda/util/cast_as.py:50), which has no missing parents\n",
      "2019-08-08 23:08:22 INFO  MemoryStore:54 - Block broadcast_31 stored as values in memory (estimated size 62.3 KB, free 361.9 MB)\n",
      "2019-08-08 23:08:22 INFO  MemoryStore:54 - Block broadcast_31_piece0 stored as bytes in memory (estimated size 26.1 KB, free 361.9 MB)\n",
      "2019-08-08 23:08:22 INFO  BlockManagerInfo:54 - Added broadcast_31_piece0 in memory on 192.168.1.33:36883 (size: 26.1 KB, free: 366.1 MB)\n",
      "2019-08-08 23:08:22 INFO  SparkContext:54 - Created broadcast 31 from broadcast at DAGScheduler.scala:1161\n",
      "2019-08-08 23:08:22 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[83] at toPandas at /home/simon/PROJECTS/pybda/pybda/util/cast_as.py:50) (first 15 tasks are for partitions Vector(0))\n",
      "2019-08-08 23:08:22 INFO  TaskSchedulerImpl:54 - Adding task set 17.0 with 1 tasks\n",
      "2019-08-08 23:08:22 INFO  TaskSetManager:54 - Starting task 0.0 in stage 17.0 (TID 17, localhost, executor driver, partition 0, PROCESS_LOCAL, 8325 bytes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-08 23:08:22 INFO  Executor:54 - Running task 0.0 in stage 17.0 (TID 17)\n",
      "2019-08-08 23:08:22 INFO  FileScanRDD:54 - Reading File path: file:///home/simon/PROJECTS/pybda/data/iris.tsv, range: 0-6740, partition values: [empty row]\n",
      "2019-08-08 23:08:22 INFO  CodeGenerator:54 - Code generated in 5.322336 ms\n",
      "2019-08-08 23:08:22 INFO  PythonRunner:54 - Times: total = 48, boot = -815, init = 859, finish = 4\n",
      "2019-08-08 23:08:22 INFO  CodeGenerator:54 - Code generated in 11.984246 ms\n",
      "2019-08-08 23:08:22 INFO  PythonRunner:54 - Times: total = 54, boot = -374, init = 426, finish = 2\n",
      "2019-08-08 23:08:22 INFO  PythonUDFRunner:54 - Times: total = 59, boot = -369, init = 420, finish = 8\n",
      "2019-08-08 23:08:22 INFO  PythonUDFRunner:54 - Times: total = 230, boot = 5, init = 60, finish = 165\n",
      "2019-08-08 23:08:22 INFO  Executor:54 - Finished task 0.0 in stage 17.0 (TID 17). 11705 bytes result sent to driver\n",
      "2019-08-08 23:08:22 INFO  TaskSetManager:54 - Finished task 0.0 in stage 17.0 (TID 17) in 273 ms on localhost (executor driver) (1/1)\n",
      "2019-08-08 23:08:22 INFO  TaskSchedulerImpl:54 - Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "2019-08-08 23:08:22 INFO  DAGScheduler:54 - ResultStage 17 (toPandas at /home/simon/PROJECTS/pybda/pybda/util/cast_as.py:50) finished in 0.279 s\n",
      "2019-08-08 23:08:22 INFO  DAGScheduler:54 - Job 16 finished: toPandas at /home/simon/PROJECTS/pybda/pybda/util/cast_as.py:50, took 0.282007 s\n",
      "2019-08-08 23:08:36 INFO  AbstractConnector:318 - Stopped Spark@6943b7b9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "2019-08-08 23:08:36 INFO  SparkUI:54 - Stopped Spark web UI at http://192.168.1.33:4040\n",
      "2019-08-08 23:08:36 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!\n",
      "2019-08-08 23:08:36 INFO  MemoryStore:54 - MemoryStore cleared\n",
      "2019-08-08 23:08:36 INFO  BlockManager:54 - BlockManager stopped\n",
      "2019-08-08 23:08:36 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped\n",
      "2019-08-08 23:08:36 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!\n",
      "2019-08-08 23:08:36 INFO  SparkContext:54 - Successfully stopped SparkContext\n",
      "2019-08-08 23:08:37 INFO  ShutdownHookManager:54 - Shutdown hook called\n",
      "2019-08-08 23:08:37 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-7f0589de-6423-43d8-9f85-f9f0fbfbe085\n",
      "2019-08-08 23:08:37 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-7f0589de-6423-43d8-9f85-f9f0fbfbe085/pyspark-9e1b4b2a-8648-49de-96f0-3687588c577b\n",
      "2019-08-08 23:08:37 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-cd2aab0c-5b54-40b5-8116-0539575fd898\n",
      "(pybda) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "cat */pca_from_iris-spark.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
