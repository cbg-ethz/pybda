
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Use case</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Spark examples" href="spark.html" />
    <link rel="prev" title="koios" href="index.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="use-case">
<h1>Use case<a class="headerlink" href="#use-case" title="Permalink to this headline">¶</a></h1>
<p>This document describes the steps taken for the data analysis of the <em>TargetInfectX</em> project.</p>
<p>## Installation information</p>
<ul class="simple">
<li>sparkhpc</li>
<li>spark</li>
<li><cite>sparksubmit</cite> in bashrc</li>
</ul>
<p>## Parsing</p>
<p>We first downloaded the complete data-set using the <cite>BeeDataDownloader</cite>.
Subsequently we do some preprocessing using our in-house python tool <cite>rnaitutilities</cite>.</p>
<p>First we check for the correct number of downloads:</p>
<dl class="docutils">
<dt><a href="#id1"><span class="problematic" id="id2">``</span></a><a href="#id3"><span class="problematic" id="id4">`</span></a>bash</dt>
<dd>rnai-parse checkdownload /cluster/home/simondi/PROJECTS/config_leonhard.yml</dd>
</dl>
<p><a href="#id5"><span class="problematic" id="id6">``</span></a><a href="#id7"><span class="problematic" id="id8">`</span></a></p>
<p>This should give some files that are <strong>not</strong> downloaded. These are indeed <strong>empty on the openBIS instance</strong>.</p>
<p>Afterwards we parse the files using:</p>
<dl class="docutils">
<dt><a href="#id9"><span class="problematic" id="id10">``</span></a><a href="#id11"><span class="problematic" id="id12">`</span></a>bash</dt>
<dd>rnai-parse parse /cluster/home/simondi/PROJECTS/config_leonhard.yml</dd>
</dl>
<p><a href="#id13"><span class="problematic" id="id14">``</span></a><a href="#id15"><span class="problematic" id="id16">`</span></a></p>
<p>Having parsed all files, we can check fif everything went as expected by creating a download report:</p>
<dl class="docutils">
<dt><a href="#id17"><span class="problematic" id="id18">``</span></a><a href="#id19"><span class="problematic" id="id20">`</span></a>bash</dt>
<dd>rnai-parse report /cluster/home/simondi/PROJECTS/config_leonhard.yml</dd>
</dl>
<p><a href="#id21"><span class="problematic" id="id22">``</span></a><a href="#id23"><span class="problematic" id="id24">`</span></a></p>
<p>Furthermore, to see which feature-sets make most sense to take, we can compute pairwise Jaccard indexes between the feature sets using:</p>
<dl class="docutils">
<dt><a href="#id25"><span class="problematic" id="id26">``</span></a><a href="#id27"><span class="problematic" id="id28">`</span></a>bash</dt>
<dd>rnai-parse featuresets /cluster/home/simondi/PROJECTS/config_leonhard.yml</dd>
</dl>
<p><a href="#id29"><span class="problematic" id="id30">``</span></a><a href="#id31"><span class="problematic" id="id32">`</span></a></p>
<p>## Preprocessing</p>
<p>Next the parsed data’s meta information are stored in a indexed data-based in
order to quickly retrieve plate information. The mentioned files are found in the
<cite>results/1-preprocessing/0-features/current_analysis</cite> folder.
The entry to this part is <cite>featuresets_feature_files.tsv</cite> which been created using <cite>rnai-parse featuresets</cite>.</p>
<p>First we created a index for the complete data-set using <cite>sqlite</cite>.
<a href="#id33"><span class="problematic" id="id34">``</span></a><a href="#id35"><span class="problematic" id="id36">`</span></a>bash</p>
<blockquote>
<div><dl class="docutils">
<dt>rnai-query insert</dt>
<dd>–db /cluster/home/simondi/simondi/data/tix/database/tix_index.db
/cluster/home/simondi/simondi/data/tix/screening_data</dd>
</dl>
</div></blockquote>
<p><a href="#id37"><span class="problematic" id="id38">``</span></a><a href="#id39"><span class="problematic" id="id40">`</span></a></p>
<p>Then create the feature sets created from calling <cite>rnai-parse featuresets</cite> (from terminal):
<a href="#id41"><span class="problematic" id="id42">``</span></a><a href="#id43"><span class="problematic" id="id44">`</span></a>bash</p>
<blockquote>
<div>./0-create_maximal_feature_sets.py featuresets_feature_files.tsv &gt; feature_sets_max.tsv</div></blockquote>
<p><a href="#id45"><span class="problematic" id="id46">``</span></a><a href="#id47"><span class="problematic" id="id48">`</span></a></p>
<p>Create plots (<cite>feature_overlap.eps</cite> and <cite>feature_histogram.eps</cite>) from the files created during the step (from terminal).
<a href="#id49"><span class="problematic" id="id50">``</span></a><a href="#id51"><span class="problematic" id="id52">`</span></a>bash</p>
<blockquote>
<div>./1-plot_featuresets.R</div></blockquote>
<p><a href="#id53"><span class="problematic" id="id54">``</span></a><a href="#id55"><span class="problematic" id="id56">`</span></a></p>
<p>Print the plates with maximal feature sets (from terminal):
<a href="#id57"><span class="problematic" id="id58">``</span></a><a href="#id59"><span class="problematic" id="id60">`</span></a>bash</p>
<blockquote>
<div><p>./2-extract_plates_from_screens.py experiment_meta_file.tsv feature_sets_max.tsv 100 &gt; feature_plates_and_screens_100.tsv</p>
<p>./2-extract_plates_from_screens.py experiment_meta_file.tsv feature_sets_max.tsv 250 &gt; feature_plates_and_screens_250.tsv</p>
<p>./2-extract_plates_from_screens.py experiment_meta_file.tsv feature_sets_max.tsv 500 &gt; feature_plates_and_screens_500.tsv</p>
</div></blockquote>
<p><a href="#id61"><span class="problematic" id="id62">``</span></a><a href="#id63"><span class="problematic" id="id64">`</span></a></p>
<p>Parse the file created above (from terminal):
<a href="#id65"><span class="problematic" id="id66">``</span></a><a href="#id67"><span class="problematic" id="id68">`</span></a>bash</p>
<blockquote>
<div>./3-plate_names.awk feature_plates_and_screens_x.tsv &gt; feature_plate_names_x.tsv</div></blockquote>
<p><a href="#id69"><span class="problematic" id="id70">``</span></a><a href="#id71"><span class="problematic" id="id72">`</span></a></p>
<p>Query the database and write result to file (since the API does not work with such a large plate list, from terminal):
<a href="#id73"><span class="problematic" id="id74">``</span></a><a href="#id75"><span class="problematic" id="id76">`</span></a>bash</p>
<blockquote>
<div>./4-get_file_sets_from_db.sh feature_plate_names_x.tsv feature_dbq_x.tsv.tsv</div></blockquote>
<p><a href="#id77"><span class="problematic" id="id78">``</span></a><a href="#id79"><span class="problematic" id="id80">`</span></a></p>
<p>The last file (<cite>feature_dbq_x.tsv</cite>) can be used with <cite>rnai-query compose</cite> to get the data from the database (from <em>leonhard</em>):
<a href="#id81"><span class="problematic" id="id82">``</span></a><a href="#id83"><span class="problematic" id="id84">`</span></a>bash</p>
<blockquote>
<div>./5-rnai_query.sh 10/100/1000 feature_dbq_250.tsv</div></blockquote>
<p><a href="#id85"><span class="problematic" id="id86">``</span></a><a href="#id87"><span class="problematic" id="id88">`</span></a></p>
<p>After that you should lpot the reults of <cite>rnai-query</cite> to make sure your data is approximately Gaussian.
I recommend to do querying on only 10 cells, too, such that plotting is easier
<a href="#id89"><span class="problematic" id="id90">``</span></a><a href="#id91"><span class="problematic" id="id92">`</span></a>bash</p>
<blockquote>
<div>./6-plot_feature_distribution.R100/1000 feature_dbq_250.tsv</div></blockquote>
<p><a href="#id93"><span class="problematic" id="id94">``</span></a><a href="#id95"><span class="problematic" id="id96">`</span></a></p>
<p><strong>This creates the data also normalizes them which are now ready for use.</strong></p>
<p>## Dimension reduction</p>
<p>Before we start analysing the data we do a dimension reduction into a 15-dimensional space using
factor analysis using <cite>1-factor_analysis-spark.py</cite> on data generated by <cite>rnai-query compile</cite>.
The respective <cite>1-factor_analysis*</cite> files are for testing.
The input file is a data set created using <cite>rnai-query compose</cite> (see above). The output is a parquet folder.</p>
<p><strong>Note that mpi and java needs to be loaded on every shell session.</strong> The job is submitted on a grid using:</p>
<p>The factor analysis can be done like this locally:
<a href="#id97"><span class="problematic" id="id98">``</span></a><a href="#id99"><span class="problematic" id="id100">`</span></a>bash</p>
<blockquote>
<div><dl class="docutils">
<dt>spark-submit –master “local[*]” –driver-memory 3G –executor-memory 6G</dt>
<dd>1-factor_analysis-spark.py
-o ./query_data/cells_sample_10_normalized_cut_100_factors
-f ./query_data/cells_sample_10_normalized_cut_100.tsv</dd>
</dl>
</div></blockquote>
<p><a href="#id101"><span class="problematic" id="id102">``</span></a><a href="#id103"><span class="problematic" id="id104">`</span></a></p>
<p>On leonhard this is the command to be executed:</p>
<dl class="docutils">
<dt><a href="#id105"><span class="problematic" id="id106">``</span></a><a href="#id107"><span class="problematic" id="id108">`</span></a>bash</dt>
<dd><p class="first">module load jdk/8u92
module load openmpi/2.1.0</p>
<p>./0a-start_cluster.sh
./0b-launch_cluster.sh &amp;</p>
<p># get master
sparkcluster info</p>
<p class="last">./1a-factor_analysis.sh MASTER &amp;</p>
</dd>
</dl>
<p><a href="#id109"><span class="problematic" id="id110">``</span></a><a href="#id111"><span class="problematic" id="id112">`</span></a></p>
<p>Afterwards the results can be visualized using:</p>
<dl class="docutils">
<dt><a href="#id113"><span class="problematic" id="id114">``</span></a><a href="#id115"><span class="problematic" id="id116">`</span></a>bash</dt>
<dd>Rscript 1b-factor_analysis_plot.R</dd>
</dl>
<p><a href="#id117"><span class="problematic" id="id118">``</span></a>`
If you also want to visualize the distribution of the components it makes sense
to run the factor analysis on a smaller data set,for instance with only 10 cells
and then run the following:</p>
<dl class="docutils">
<dt><a href="#id119"><span class="problematic" id="id120">``</span></a><a href="#id121"><span class="problematic" id="id122">`</span></a>bash</dt>
<dd>2a-parquet_to_tsv.sh
2b-plot_feature_distributions.R</dd>
</dl>
<p><a href="#id123"><span class="problematic" id="id124">``</span></a><a href="#id125"><span class="problematic" id="id126">`</span></a></p>
<p>Finally we remove some outliers. As before we are assuming <strong>GAUSSIAN</strong> features,
so the output of <cite>2b-plot_feature_distributions.R</cite> should be approximately normal
(or whatever).</p>
<p>Locally you would call:
<a href="#id127"><span class="problematic" id="id128">``</span></a><a href="#id129"><span class="problematic" id="id130">`</span></a>bash</p>
<blockquote>
<div><dl class="docutils">
<dt>spark-submit –master “local[*]” –driver-memory 3G –executor-memory 6G</dt>
<dd>3-outlier_removal.py
-o ./query_data/cells_sample_10_normalized_cut_100_factors
-f ./query_data/cells_sample_10_normalized_cut_100.tsv</dd>
</dl>
</div></blockquote>
<p><a href="#id131"><span class="problematic" id="id132">``</span></a><a href="#id133"><span class="problematic" id="id134">`</span></a></p>
<p>On leonhard this is the command to be executed:</p>
<dl class="docutils">
<dt><a href="#id135"><span class="problematic" id="id136">``</span></a><a href="#id137"><span class="problematic" id="id138">`</span></a>bash</dt>
<dd><p class="first">module load jdk/8u92
module load openmpi/2.1.0</p>
<p>./0a-start_cluster.sh
./0b-launch_cluster.sh &amp;</p>
<p># get master
sparkcluster info</p>
<p class="last">./3a-remove_outliers.sh MASTER &amp;</p>
</dd>
</dl>
<p><a href="#id139"><span class="problematic" id="id140">``</span></a><a href="#id141"><span class="problematic" id="id142">`</span></a></p>
<p>## Analysis</p>
<p>### Clustering</p>
<p><cite>1-kmeans_spark.py</cite> clusters data generated using the previous factor analysis (or raw data). The respective
<cite>1-kmeans_spark.ipynp</cite> is a trial-and-error script for testing.
The input file and output folder should be always the same, for example <em>cells_sample_10_100lines.tsv</em>  as input file or the output folder created by the factor analysis (cells_sample_10_normalized_cut_100_factors) and some folder <em>out</em> as output.</p>
<p>First run the script using <cite>fit</cite> on a couple of different cluster centers`k`s,
then plot the results to determine how many cluster centers you need and
finally transform the data with the respective <cite>k</cite>.</p>
<p>The clustering can be done like this locally:
<a href="#id143"><span class="problematic" id="id144">``</span></a><a href="#id145"><span class="problematic" id="id146">`</span></a>bash</p>
<blockquote>
<div><p>for i in {2..15};
do</p>
<blockquote>
<div><dl class="docutils">
<dt>spark-submit –master “local[*]” –driver-memory 3G –executor-memory 6G </dt>
<dd>1-kmeans_spark.py -o ./1-clustering/test -f ./query_data/cells_sample_10_normalized_cut_100_factors fit -k ${i}</dd>
</dl>
</div></blockquote>
<p>done</p>
<dl class="docutils">
<dt>spark-submit –master “local[*]” –driver-memory 3G –executor-memory 6G </dt>
<dd>1-kmeans_spark.py -o ./1-clustering/test -f ./query_data/cells_sample_10_normalized_cut_100_factors plot</dd>
<dt>spark-submit –master “local[*]” –driver-memory 3G –executor-memory 6G </dt>
<dd>1-kmeans_spark.py -o ./1-clustering/test -f ./query_data/cells_sample_10_normalized_cut_100_factors transform -k BEST_K_FROM_PLOT</dd>
<dt>spark-submit –master “local[*]” –driver-memory 3G –executor-memory 6G </dt>
<dd>1-kmeans_statistics-spark.py -f ./1-clustering/test/kmeans_transform-cells_sample_10_normalized_cut_100_factors_K005</dd>
</dl>
</div></blockquote>
<p><a href="#id147"><span class="problematic" id="id148">``</span></a><a href="#id149"><span class="problematic" id="id150">`</span></a></p>
<p><strong>Note that mpi and java needs to be loaded on every shell session.</strong> The job is submitted on a grid using:</p>
<dl class="docutils">
<dt><a href="#id151"><span class="problematic" id="id152">``</span></a><a href="#id153"><span class="problematic" id="id154">`</span></a>bash</dt>
<dd><p class="first">module load jdk/8u92
module load openmpi/2.1.0</p>
<p>./1a-start_cluster.sh
./1b-launch_cluster.sh &amp;</p>
<p># get master
sparkcluster info</p>
<p>./1a-kmeans-fit-all.sh spark:master K
./1b-kmeans-plot.sh spark:master
./1c-kmeans-transform.sh spark:master K</p>
<p class="last">./1d-kmeans-statistics spark:master kmeans_transformed_folder</p>
</dd>
</dl>
<p><a href="#id155"><span class="problematic" id="id156">``</span></a><a href="#id157"><span class="problematic" id="id158">`</span></a></p>
<p>Then plot the results from the statistics and the clusterings.</p>
<dl class="docutils">
<dt><a href="#id159"><span class="problematic" id="id160">``</span></a><a href="#id161"><span class="problematic" id="id162">`</span></a>bash</dt>
<dd>Rscript 1e-kmeans-statistics_plot.R
Rscript 1g-kmeans-plot_clusters.R</dd>
</dl>
<p><a href="#id163"><span class="problematic" id="id164">``</span></a><a href="#id165"><span class="problematic" id="id166">`</span></a></p>
<p><strong>The other script `1f-kmeans-sample` has not been very much used to far.</strong></p>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><p class="logo">
  <a href="index.html">
    <img src="_static/biospark_logo.png"
         title="biospark"/>
  </a>
</p><h3>Contents</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">Home</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Use case</a></li>
<li class="toctree-l1"><a class="reference internal" href="spark.html">Spark examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">TargetInfectX data analysis</a></li>
</ul>

        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2018, Simon Dirmeier.
      
    </div>

    

    
  </body>
</html>