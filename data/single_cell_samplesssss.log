[2018-09-07 22:17:45,403 - INFO - __main__]: Initializing pyspark session
[2018-09-07 22:17:49,234 - INFO - __main__]: Config: spark.driver.memory, value: 3G
[2018-09-07 22:17:49,234 - INFO - __main__]: Config: spark.master, value: spark://bs-mbpr134.d.ethz.ch:7077
[2018-09-07 22:17:49,234 - INFO - __main__]: Config: spark.driver.port, value: 59911
[2018-09-07 22:17:49,234 - INFO - __main__]: Config: spark.executor.memory, value: 6G
[2018-09-07 22:17:49,234 - INFO - __main__]: Config: spark.files, value: file:/Users/simondi/PROJECTS/target_infect_x_project/src/tix-analysis/koios/sampler.py
[2018-09-07 22:17:49,234 - INFO - __main__]: Config: spark.rdd.compress, value: True
[2018-09-07 22:17:49,234 - INFO - __main__]: Config: spark.app.name, value: sampler.py
[2018-09-07 22:17:49,234 - INFO - __main__]: Config: spark.serializer.objectStreamReset, value: 100
[2018-09-07 22:17:49,234 - INFO - __main__]: Config: spark.executor.id, value: driver
[2018-09-07 22:17:49,235 - INFO - __main__]: Config: spark.submit.deployMode, value: client
[2018-09-07 22:17:49,235 - INFO - __main__]: Config: spark.driver.host, value: bs-mbpr134.default
[2018-09-07 22:17:49,235 - INFO - __main__]: Config: spark.app.id, value: app-20180907221747-0000
[2018-09-07 22:17:49,236 - INFO - __main__]: Found suffix 'tsv', expecting tsv file as input
[2018-09-07 22:17:49,237 - INFO - __main__]: Writing parquet as output
[2018-09-07 22:17:49,237 - INFO - koios.io.io]: Reading  tsv: data/single_cell_samples.tsv
[2018-09-07 22:18:29,342 - INFO - __main__]: Initializing pyspark session
[2018-09-07 22:18:32,848 - INFO - __main__]: Config: spark.driver.memory, value: 3G
[2018-09-07 22:18:32,848 - INFO - __main__]: Config: spark.master, value: spark://bs-mbpr134.d.ethz.ch:7077
[2018-09-07 22:18:32,848 - INFO - __main__]: Config: spark.executor.memory, value: 6G
[2018-09-07 22:18:32,848 - INFO - __main__]: Config: spark.files, value: file:/Users/simondi/PROJECTS/target_infect_x_project/src/tix-analysis/koios/sampler.py
[2018-09-07 22:18:32,848 - INFO - __main__]: Config: spark.app.id, value: app-20180907221831-0001
[2018-09-07 22:18:32,849 - INFO - __main__]: Config: spark.rdd.compress, value: True
[2018-09-07 22:18:32,849 - INFO - __main__]: Config: spark.driver.port, value: 59923
[2018-09-07 22:18:32,849 - INFO - __main__]: Config: spark.app.name, value: sampler.py
[2018-09-07 22:18:32,849 - INFO - __main__]: Config: spark.serializer.objectStreamReset, value: 100
[2018-09-07 22:18:32,849 - INFO - __main__]: Config: spark.executor.id, value: driver
[2018-09-07 22:18:32,849 - INFO - __main__]: Config: spark.submit.deployMode, value: client
[2018-09-07 22:18:32,849 - INFO - __main__]: Config: spark.driver.host, value: bs-mbpr134.default
[2018-09-07 22:18:32,850 - INFO - __main__]: Found suffix 'tsv', expecting tsv file as input
[2018-09-07 22:18:32,850 - INFO - __main__]: Writing parquet as output
[2018-09-07 22:18:32,851 - INFO - koios.io.io]: Reading  tsv: data/single_cell_samples.tsv
[2018-09-07 22:19:30,535 - INFO - __main__]: Initializing pyspark session
[2018-09-07 22:19:34,989 - INFO - __main__]: Config: spark.driver.memory, value: 3G
[2018-09-07 22:19:34,989 - INFO - __main__]: Config: spark.master, value: spark://bs-mbpr134.d.ethz.ch:7077
[2018-09-07 22:19:34,991 - INFO - __main__]: Config: spark.app.id, value: app-20180907221933-0002
[2018-09-07 22:19:34,991 - INFO - __main__]: Config: spark.executor.memory, value: 6G
[2018-09-07 22:19:34,991 - INFO - __main__]: Config: spark.files, value: file:/Users/simondi/PROJECTS/target_infect_x_project/src/tix-analysis/koios/sampler.py
[2018-09-07 22:19:34,992 - INFO - __main__]: Config: spark.rdd.compress, value: True
[2018-09-07 22:19:34,992 - INFO - __main__]: Config: spark.app.name, value: sampler.py
[2018-09-07 22:19:34,992 - INFO - __main__]: Config: spark.serializer.objectStreamReset, value: 100
[2018-09-07 22:19:34,992 - INFO - __main__]: Config: spark.executor.id, value: driver
[2018-09-07 22:19:34,992 - INFO - __main__]: Config: spark.submit.deployMode, value: client
[2018-09-07 22:19:34,992 - INFO - __main__]: Config: spark.driver.host, value: bs-mbpr134.default
[2018-09-07 22:19:34,992 - INFO - __main__]: Config: spark.driver.port, value: 59937
[2018-09-07 22:19:34,994 - INFO - __main__]: Found suffix 'tsv', expecting tsv file as input
[2018-09-07 22:19:34,994 - INFO - __main__]: Writing parquet as output
[2018-09-07 22:19:34,994 - INFO - koios.io.io]: Reading  tsv: data/single_cell_samples.tsv
[2018-09-07 22:19:49,516 - INFO - koios.io.io]: Writing parquet: data/single_cell_samplesssss
[2018-09-07 22:19:51,860 - INFO - __main__]: Stopping Spark context
[2018-09-07 22:22:35,166 - INFO - __main__]: Initializing pyspark session
[2018-09-07 22:22:38,622 - INFO - __main__]: Config: spark.driver.memory, value: 3G
[2018-09-07 22:22:38,623 - INFO - __main__]: Config: spark.master, value: spark://bs-mbpr134.d.ethz.ch:7077
[2018-09-07 22:22:38,623 - INFO - __main__]: Config: spark.executor.memory, value: 6G
[2018-09-07 22:22:38,623 - INFO - __main__]: Config: spark.files, value: file:/Users/simondi/PROJECTS/target_infect_x_project/src/tix-analysis/koios/sampler.py
[2018-09-07 22:22:38,624 - INFO - __main__]: Config: spark.rdd.compress, value: True
[2018-09-07 22:22:38,624 - INFO - __main__]: Config: spark.driver.port, value: 59992
[2018-09-07 22:22:38,624 - INFO - __main__]: Config: spark.app.name, value: sampler.py
[2018-09-07 22:22:38,624 - INFO - __main__]: Config: spark.serializer.objectStreamReset, value: 100
[2018-09-07 22:22:38,625 - INFO - __main__]: Config: spark.executor.id, value: driver
[2018-09-07 22:22:38,625 - INFO - __main__]: Config: spark.submit.deployMode, value: client
[2018-09-07 22:22:38,625 - INFO - __main__]: Config: spark.driver.host, value: bs-mbpr134.default
[2018-09-07 22:22:38,625 - INFO - __main__]: Config: spark.app.id, value: app-20180907222237-0003
[2018-09-07 22:22:38,629 - INFO - __main__]: Found suffix 'tsv', expecting tsv file as input
[2018-09-07 22:22:38,630 - INFO - __main__]: Found suffix 'tsv', writing tsv file as output
[2018-09-07 22:22:38,631 - INFO - koios.io.io]: Reading  tsv: data/single_cell_samples.tsv
[2018-09-07 22:23:20,111 - INFO - __main__]: Initializing pyspark session
[2018-09-07 22:23:22,958 - INFO - __main__]: Config: spark.driver.memory, value: 3G
[2018-09-07 22:23:22,958 - INFO - __main__]: Config: spark.master, value: spark://bs-mbpr134.d.ethz.ch:7077
[2018-09-07 22:23:22,958 - INFO - __main__]: Config: spark.executor.memory, value: 6G
[2018-09-07 22:23:22,958 - INFO - __main__]: Config: spark.files, value: file:/Users/simondi/PROJECTS/target_infect_x_project/src/tix-analysis/koios/sampler.py
[2018-09-07 22:23:22,958 - INFO - __main__]: Config: spark.rdd.compress, value: True
[2018-09-07 22:23:22,958 - INFO - __main__]: Config: spark.app.name, value: sampler.py
[2018-09-07 22:23:22,959 - INFO - __main__]: Config: spark.serializer.objectStreamReset, value: 100
[2018-09-07 22:23:22,959 - INFO - __main__]: Config: spark.executor.id, value: driver
[2018-09-07 22:23:22,959 - INFO - __main__]: Config: spark.submit.deployMode, value: client
[2018-09-07 22:23:22,959 - INFO - __main__]: Config: spark.driver.host, value: bs-mbpr134.default
[2018-09-07 22:23:22,959 - INFO - __main__]: Config: spark.driver.port, value: 60014
[2018-09-07 22:23:22,959 - INFO - __main__]: Config: spark.app.id, value: app-20180907222322-0004
[2018-09-07 22:23:22,960 - INFO - __main__]: Found suffix 'tsv', expecting tsv file as input
[2018-09-07 22:23:22,960 - INFO - __main__]: Found suffix 'tsv', writing tsv file as output
[2018-09-07 22:23:22,960 - INFO - koios.io.io]: Reading  tsv: data/single_cell_samples.tsv
[2018-09-07 22:25:14,295 - INFO - __main__]: Initializing pyspark session
[2018-09-07 22:25:19,931 - INFO - __main__]: Config: spark.driver.memory, value: 3G
[2018-09-07 22:25:19,931 - INFO - __main__]: Config: spark.master, value: spark://bs-mbpr134.d.ethz.ch:7077
[2018-09-07 22:25:19,932 - INFO - __main__]: Config: spark.executor.memory, value: 6G
[2018-09-07 22:25:19,932 - INFO - __main__]: Config: spark.files, value: file:/Users/simondi/PROJECTS/target_infect_x_project/src/tix-analysis/koios/sampler.py
[2018-09-07 22:25:19,932 - INFO - __main__]: Config: spark.rdd.compress, value: True
[2018-09-07 22:25:19,932 - INFO - __main__]: Config: spark.app.name, value: sampler.py
[2018-09-07 22:25:19,932 - INFO - __main__]: Config: spark.serializer.objectStreamReset, value: 100
[2018-09-07 22:25:19,932 - INFO - __main__]: Config: spark.executor.id, value: driver
[2018-09-07 22:25:19,932 - INFO - __main__]: Config: spark.submit.deployMode, value: client
[2018-09-07 22:25:19,932 - INFO - __main__]: Config: spark.app.id, value: app-20180907222519-0005
[2018-09-07 22:25:19,933 - INFO - __main__]: Config: spark.driver.host, value: bs-mbpr134.default
[2018-09-07 22:25:19,933 - INFO - __main__]: Config: spark.driver.port, value: 60039
[2018-09-07 22:25:19,934 - INFO - __main__]: Found suffix 'tsv', expecting tsv file as input
[2018-09-07 22:25:19,934 - INFO - __main__]: Found suffix 'tsv', writing tsv file as output
[2018-09-07 22:25:19,934 - INFO - koios.io.io]: Reading  tsv: data/single_cell_samples.tsv
[2018-09-07 22:26:24,645 - INFO - __main__]: Initializing pyspark session
[2018-09-07 22:26:35,957 - INFO - __main__]: Config: spark.driver.memory, value: 3G
[2018-09-07 22:26:35,957 - INFO - __main__]: Config: spark.master, value: spark://bs-mbpr134.d.ethz.ch:7077
[2018-09-07 22:26:35,957 - INFO - __main__]: Config: spark.executor.memory, value: 6G
[2018-09-07 22:26:35,958 - INFO - __main__]: Config: spark.files, value: file:/Users/simondi/PROJECTS/target_infect_x_project/src/tix-analysis/koios/sampler.py
[2018-09-07 22:26:35,958 - INFO - __main__]: Config: spark.rdd.compress, value: True
[2018-09-07 22:26:35,958 - INFO - __main__]: Config: spark.app.name, value: sampler.py
[2018-09-07 22:26:35,958 - INFO - __main__]: Config: spark.serializer.objectStreamReset, value: 100
[2018-09-07 22:26:35,958 - INFO - __main__]: Config: spark.executor.id, value: driver
[2018-09-07 22:26:35,958 - INFO - __main__]: Config: spark.submit.deployMode, value: client
[2018-09-07 22:26:35,958 - INFO - __main__]: Config: spark.driver.port, value: 60106
[2018-09-07 22:26:35,958 - INFO - __main__]: Config: spark.driver.host, value: bs-mbpr134.default
[2018-09-07 22:26:35,958 - INFO - __main__]: Config: spark.app.id, value: app-20180907222632-0006
[2018-09-07 22:26:35,960 - INFO - __main__]: Found suffix 'tsv', expecting tsv file as input
[2018-09-07 22:26:35,960 - INFO - __main__]: Found suffix 'tsv', writing tsv file as output
[2018-09-07 22:26:35,961 - INFO - koios.io.io]: Reading  tsv: data/single_cell_samples.tsv
[2018-09-07 22:35:43,266 - INFO - __main__]: Initializing pyspark session
[2018-09-07 22:35:47,502 - INFO - __main__]: Config: spark.driver.memory, value: 3G
[2018-09-07 22:35:47,503 - INFO - __main__]: Config: spark.master, value: spark://bs-mbpr134.d.ethz.ch:7077
[2018-09-07 22:35:47,503 - INFO - __main__]: Config: spark.executor.memory, value: 6G
[2018-09-07 22:35:47,503 - INFO - __main__]: Config: spark.files, value: file:/Users/simondi/PROJECTS/target_infect_x_project/src/tix-analysis/koios/sampler.py
[2018-09-07 22:35:47,503 - INFO - __main__]: Config: spark.rdd.compress, value: True
[2018-09-07 22:35:47,503 - INFO - __main__]: Config: spark.app.name, value: sampler.py
[2018-09-07 22:35:47,503 - INFO - __main__]: Config: spark.serializer.objectStreamReset, value: 100
[2018-09-07 22:35:47,504 - INFO - __main__]: Config: spark.executor.id, value: driver
[2018-09-07 22:35:47,504 - INFO - __main__]: Config: spark.submit.deployMode, value: client
[2018-09-07 22:35:47,504 - INFO - __main__]: Config: spark.driver.port, value: 60330
[2018-09-07 22:35:47,504 - INFO - __main__]: Config: spark.driver.host, value: bs-mbpr134.default
[2018-09-07 22:35:47,504 - INFO - __main__]: Config: spark.app.id, value: app-20180907223546-0007
[2018-09-07 22:35:47,507 - INFO - __main__]: Found suffix 'tsv', expecting tsv file as input
[2018-09-07 22:35:47,507 - INFO - __main__]: Found suffix 'tsv', writing tsv file as output
[2018-09-07 22:35:47,507 - INFO - koios.io.io]: Reading  tsv: data/single_cell_samples.tsv
[2018-09-07 22:36:01,432 - INFO - koios.util.features]: Vector columns 'features' not found. Returning
[2018-09-07 22:36:01,433 - INFO - koios.io.io]: Writing tsv: data/single_cell_samplesssss.tsv
[2018-09-07 22:36:02,393 - INFO - __main__]: Stopping Spark context
[2018-09-07 22:37:23,457 - INFO - __main__]: Initializing pyspark session
[2018-09-07 22:37:28,297 - INFO - __main__]: Config: spark.driver.memory, value: 3G
[2018-09-07 22:37:28,297 - INFO - __main__]: Config: spark.master, value: spark://bs-mbpr134.d.ethz.ch:7077
[2018-09-07 22:37:28,297 - INFO - __main__]: Config: spark.app.id, value: app-20180907223726-0008
[2018-09-07 22:37:28,298 - INFO - __main__]: Config: spark.executor.memory, value: 6G
[2018-09-07 22:37:28,298 - INFO - __main__]: Config: spark.driver.port, value: 60343
[2018-09-07 22:37:28,298 - INFO - __main__]: Config: spark.files, value: file:/Users/simondi/PROJECTS/target_infect_x_project/src/tix-analysis/koios/sampler.py
[2018-09-07 22:37:28,298 - INFO - __main__]: Config: spark.rdd.compress, value: True
[2018-09-07 22:37:28,298 - INFO - __main__]: Config: spark.app.name, value: sampler.py
[2018-09-07 22:37:28,298 - INFO - __main__]: Config: spark.serializer.objectStreamReset, value: 100
[2018-09-07 22:37:28,298 - INFO - __main__]: Config: spark.executor.id, value: driver
[2018-09-07 22:37:28,298 - INFO - __main__]: Config: spark.submit.deployMode, value: client
[2018-09-07 22:37:28,298 - INFO - __main__]: Config: spark.driver.host, value: bs-mbpr134.default
[2018-09-07 22:37:28,301 - INFO - __main__]: Found suffix 'tsv', expecting tsv file as input
[2018-09-07 22:37:28,301 - INFO - __main__]: Found suffix 'tsv', writing tsv file as output
[2018-09-07 22:37:28,301 - INFO - koios.io.io]: Reading  tsv: data/single_cell_samples.tsv
[2018-09-07 22:37:38,672 - INFO - __main__]: Initializing pyspark session
[2018-09-07 22:37:41,423 - INFO - __main__]: Config: spark.driver.memory, value: 3G
[2018-09-07 22:37:41,423 - INFO - __main__]: Config: spark.master, value: spark://bs-mbpr134.d.ethz.ch:7077
[2018-09-07 22:37:41,423 - INFO - __main__]: Config: spark.executor.memory, value: 6G
[2018-09-07 22:37:41,423 - INFO - __main__]: Config: spark.files, value: file:/Users/simondi/PROJECTS/target_infect_x_project/src/tix-analysis/koios/sampler.py
[2018-09-07 22:37:41,423 - INFO - __main__]: Config: spark.driver.port, value: 60351
[2018-09-07 22:37:41,424 - INFO - __main__]: Config: spark.rdd.compress, value: True
[2018-09-07 22:37:41,424 - INFO - __main__]: Config: spark.app.name, value: sampler.py
[2018-09-07 22:37:41,424 - INFO - __main__]: Config: spark.serializer.objectStreamReset, value: 100
[2018-09-07 22:37:41,424 - INFO - __main__]: Config: spark.executor.id, value: driver
[2018-09-07 22:37:41,424 - INFO - __main__]: Config: spark.submit.deployMode, value: client
[2018-09-07 22:37:41,424 - INFO - __main__]: Config: spark.app.id, value: app-20180907223740-0009
[2018-09-07 22:37:41,424 - INFO - __main__]: Config: spark.driver.host, value: bs-mbpr134.default
[2018-09-07 22:37:41,427 - INFO - __main__]: Found suffix 'tsv', expecting tsv file as input
[2018-09-07 22:37:41,427 - INFO - __main__]: Found suffix 'tsv', writing tsv file as output
[2018-09-07 22:37:41,428 - INFO - koios.io.io]: Reading  tsv: data/single_cell_samples.tsv
[2018-09-07 22:37:51,608 - INFO - koios.util.features]: Vector columns 'features' not found. Returning
[2018-09-07 22:37:51,608 - INFO - koios.io.io]: Writing tsv: data/single_cell_samplesssss.tsv
[2018-09-07 22:37:52,711 - INFO - __main__]: Stopping Spark context
[2018-09-07 22:46:48,436 - INFO - __main__]: Initializing pyspark session
[2018-09-07 22:46:50,974 - INFO - __main__]: Config: spark.driver.memory, value: 3G
[2018-09-07 22:46:50,974 - INFO - __main__]: Config: spark.master, value: spark://bs-mbpr134.d.ethz.ch:7077
[2018-09-07 22:46:50,974 - INFO - __main__]: Config: spark.executor.memory, value: 6G
[2018-09-07 22:46:50,974 - INFO - __main__]: Config: spark.files, value: file:/Users/simondi/PROJECTS/target_infect_x_project/src/tix-analysis/koios/sampler.py
[2018-09-07 22:46:50,975 - INFO - __main__]: Config: spark.rdd.compress, value: True
[2018-09-07 22:46:50,975 - INFO - __main__]: Config: spark.app.name, value: sampler.py
[2018-09-07 22:46:50,975 - INFO - __main__]: Config: spark.serializer.objectStreamReset, value: 100
[2018-09-07 22:46:50,975 - INFO - __main__]: Config: spark.executor.id, value: driver
[2018-09-07 22:46:50,975 - INFO - __main__]: Config: spark.submit.deployMode, value: client
[2018-09-07 22:46:50,975 - INFO - __main__]: Config: spark.driver.host, value: bs-mbpr134.default
[2018-09-07 22:46:50,975 - INFO - __main__]: Config: spark.driver.port, value: 60402
[2018-09-07 22:46:50,975 - INFO - __main__]: Config: spark.app.id, value: app-20180907224650-0010
[2018-09-07 22:48:04,233 - INFO - __main__]: Initializing pyspark session
[2018-09-07 22:48:07,255 - INFO - __main__]: Config: spark.driver.memory, value: 3G
[2018-09-07 22:48:07,255 - INFO - __main__]: Config: spark.master, value: spark://bs-mbpr134.d.ethz.ch:7077
[2018-09-07 22:48:07,255 - INFO - __main__]: Config: spark.app.id, value: app-20180907224805-0011
[2018-09-07 22:48:07,255 - INFO - __main__]: Config: spark.executor.memory, value: 6G
[2018-09-07 22:48:07,255 - INFO - __main__]: Config: spark.files, value: file:/Users/simondi/PROJECTS/target_infect_x_project/src/tix-analysis/koios/sampler.py
[2018-09-07 22:48:07,255 - INFO - __main__]: Config: spark.rdd.compress, value: True
[2018-09-07 22:48:07,255 - INFO - __main__]: Config: spark.app.name, value: sampler.py
[2018-09-07 22:48:07,255 - INFO - __main__]: Config: spark.serializer.objectStreamReset, value: 100
[2018-09-07 22:48:07,255 - INFO - __main__]: Config: spark.driver.port, value: 60426
[2018-09-07 22:48:07,255 - INFO - __main__]: Config: spark.executor.id, value: driver
[2018-09-07 22:48:07,256 - INFO - __main__]: Config: spark.submit.deployMode, value: client
[2018-09-07 22:48:07,256 - INFO - __main__]: Config: spark.driver.host, value: bs-mbpr134.default
[2018-09-07 22:48:07,257 - INFO - __main__]: Found folder, expecting parquet file as input
[2018-09-07 22:48:07,257 - INFO - __main__]: Found suffix 'tsv', writing RDDs as tsvs
[2018-09-07 22:49:21,420 - INFO - __main__]: Initializing pyspark session
[2018-09-07 22:49:26,911 - INFO - __main__]: Config: spark.driver.memory, value: 3G
[2018-09-07 22:49:26,912 - INFO - __main__]: Config: spark.master, value: spark://bs-mbpr134.d.ethz.ch:7077
[2018-09-07 22:49:26,913 - INFO - __main__]: Config: spark.executor.memory, value: 6G
[2018-09-07 22:49:26,914 - INFO - __main__]: Config: spark.driver.port, value: 60437
[2018-09-07 22:49:26,914 - INFO - __main__]: Config: spark.files, value: file:/Users/simondi/PROJECTS/target_infect_x_project/src/tix-analysis/koios/sampler.py
[2018-09-07 22:49:26,914 - INFO - __main__]: Config: spark.rdd.compress, value: True
[2018-09-07 22:49:26,914 - INFO - __main__]: Config: spark.app.name, value: sampler.py
[2018-09-07 22:49:26,914 - INFO - __main__]: Config: spark.serializer.objectStreamReset, value: 100
[2018-09-07 22:49:26,914 - INFO - __main__]: Config: spark.app.id, value: app-20180907224925-0012
[2018-09-07 22:49:26,915 - INFO - __main__]: Config: spark.executor.id, value: driver
[2018-09-07 22:49:26,915 - INFO - __main__]: Config: spark.submit.deployMode, value: client
[2018-09-07 22:49:26,915 - INFO - __main__]: Config: spark.driver.host, value: bs-mbpr134.default
[2018-09-07 22:49:26,919 - INFO - __main__]: Found folder, expecting parquet file as input
[2018-09-07 22:49:26,919 - INFO - __main__]: Found suffix 'tsv', writing RDDs as tsvs
[2018-09-07 22:49:26,919 - INFO - koios.io.io]: Reading parquet folder: data/factor_analysis
[2018-09-07 22:49:40,597 - INFO - koios.io.io]: Writing tsv: data/single_cell_samplesssss
[2018-09-07 22:53:44,183 - INFO - __main__]: Initializing pyspark session
[2018-09-07 22:53:47,089 - INFO - __main__]: Config: spark.driver.memory, value: 3G
[2018-09-07 22:53:47,089 - INFO - __main__]: Config: spark.master, value: spark://bs-mbpr134.d.ethz.ch:7077
[2018-09-07 22:53:47,089 - INFO - __main__]: Config: spark.executor.memory, value: 6G
[2018-09-07 22:53:47,089 - INFO - __main__]: Config: spark.app.id, value: app-20180907225346-0013
[2018-09-07 22:53:47,089 - INFO - __main__]: Config: spark.files, value: file:/Users/simondi/PROJECTS/target_infect_x_project/src/tix-analysis/koios/sampler.py
[2018-09-07 22:53:47,089 - INFO - __main__]: Config: spark.rdd.compress, value: True
[2018-09-07 22:53:47,089 - INFO - __main__]: Config: spark.app.name, value: sampler.py
[2018-09-07 22:53:47,089 - INFO - __main__]: Config: spark.serializer.objectStreamReset, value: 100
[2018-09-07 22:53:47,089 - INFO - __main__]: Config: spark.executor.id, value: driver
[2018-09-07 22:53:47,090 - INFO - __main__]: Config: spark.submit.deployMode, value: client
[2018-09-07 22:53:47,090 - INFO - __main__]: Config: spark.driver.host, value: bs-mbpr134.default
[2018-09-07 22:53:47,090 - INFO - __main__]: Config: spark.driver.port, value: 60495
[2018-09-07 22:53:47,092 - INFO - __main__]: Found folder, expecting parquet file as input
[2018-09-07 22:53:47,092 - INFO - __main__]: Found suffix 'tsv', writing RDDs as tsvs
[2018-09-07 22:53:47,092 - INFO - koios.io.io]: Reading parquet folder: data/factor_analysis
[2018-09-07 22:53:56,235 - INFO - koios.io.io]: Writing tsv: data/single_cell_samplesssss
[2018-09-07 22:54:59,114 - INFO - __main__]: Initializing pyspark session
[2018-09-07 22:55:02,102 - INFO - __main__]: Config: spark.driver.memory, value: 3G
[2018-09-07 22:55:02,102 - INFO - __main__]: Config: spark.master, value: spark://bs-mbpr134.d.ethz.ch:7077
[2018-09-07 22:55:02,102 - INFO - __main__]: Config: spark.app.id, value: app-20180907225500-0014
[2018-09-07 22:55:02,103 - INFO - __main__]: Config: spark.executor.memory, value: 6G
[2018-09-07 22:55:02,103 - INFO - __main__]: Config: spark.files, value: file:/Users/simondi/PROJECTS/target_infect_x_project/src/tix-analysis/koios/sampler.py
[2018-09-07 22:55:02,103 - INFO - __main__]: Config: spark.driver.port, value: 60511
[2018-09-07 22:55:02,103 - INFO - __main__]: Config: spark.rdd.compress, value: True
[2018-09-07 22:55:02,103 - INFO - __main__]: Config: spark.app.name, value: sampler.py
[2018-09-07 22:55:02,103 - INFO - __main__]: Config: spark.serializer.objectStreamReset, value: 100
[2018-09-07 22:55:02,103 - INFO - __main__]: Config: spark.executor.id, value: driver
[2018-09-07 22:55:02,103 - INFO - __main__]: Config: spark.submit.deployMode, value: client
[2018-09-07 22:55:02,103 - INFO - __main__]: Config: spark.driver.host, value: bs-mbpr134.default
[2018-09-07 22:55:02,104 - INFO - __main__]: Found folder, expecting parquet file as input
[2018-09-07 22:55:02,105 - INFO - __main__]: Found suffix 'tsv', writing RDDs as tsvs
[2018-09-07 22:55:02,105 - INFO - koios.io.io]: Reading parquet folder: data/factor_analysis
[2018-09-07 22:55:11,066 - INFO - koios.io.io]: Writing tsv: data/single_cell_samplesssss
[2018-09-07 22:55:11,281 - ERROR - __main__]: Some error: An error occurred while calling o70.csv.
: java.lang.UnsupportedOperationException: CSV data source does not support struct<type:tinyint,size:int,indices:array<int>,values:array<double>> data type.
	at org.apache.spark.sql.execution.datasources.csv.CSVUtils$.org$apache$spark$sql$execution$datasources$csv$CSVUtils$$verifyType$1(CSVUtils.scala:127)
	at org.apache.spark.sql.execution.datasources.csv.CSVUtils$$anonfun$verifySchema$1.apply(CSVUtils.scala:131)
	at org.apache.spark.sql.execution.datasources.csv.CSVUtils$$anonfun$verifySchema$1.apply(CSVUtils.scala:131)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)
	at org.apache.spark.sql.execution.datasources.csv.CSVUtils$.verifySchema(CSVUtils.scala:131)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.prepareWrite(CSVFileFormat.scala:65)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:140)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:154)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)
	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:642)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)

[2018-09-07 22:55:11,282 - INFO - __main__]: Stopping Spark context
[2018-09-07 22:56:20,991 - INFO - __main__]: Initializing pyspark session
[2018-09-07 22:56:24,051 - INFO - __main__]: Config: spark.driver.memory, value: 3G
[2018-09-07 22:56:24,051 - INFO - __main__]: Config: spark.master, value: spark://bs-mbpr134.d.ethz.ch:7077
[2018-09-07 22:56:24,051 - INFO - __main__]: Config: spark.executor.memory, value: 6G
[2018-09-07 22:56:24,051 - INFO - __main__]: Config: spark.files, value: file:/Users/simondi/PROJECTS/target_infect_x_project/src/tix-analysis/koios/sampler.py
[2018-09-07 22:56:24,051 - INFO - __main__]: Config: spark.driver.port, value: 60524
[2018-09-07 22:56:24,051 - INFO - __main__]: Config: spark.rdd.compress, value: True
[2018-09-07 22:56:24,051 - INFO - __main__]: Config: spark.app.name, value: sampler.py
[2018-09-07 22:56:24,051 - INFO - __main__]: Config: spark.serializer.objectStreamReset, value: 100
[2018-09-07 22:56:24,051 - INFO - __main__]: Config: spark.executor.id, value: driver
[2018-09-07 22:56:24,052 - INFO - __main__]: Config: spark.submit.deployMode, value: client
[2018-09-07 22:56:24,052 - INFO - __main__]: Config: spark.driver.host, value: bs-mbpr134.default
[2018-09-07 22:56:24,052 - INFO - __main__]: Config: spark.app.id, value: app-20180907225623-0015
[2018-09-07 22:56:24,053 - INFO - __main__]: Found folder, expecting parquet file as input
[2018-09-07 22:56:24,053 - INFO - __main__]: Found suffix 'tsv', writing RDDs as tsvs
[2018-09-07 22:56:24,053 - INFO - koios.io.io]: Reading parquet folder: data/factor_analysis
[2018-09-07 22:56:33,899 - INFO - koios.util.features]: Splitting vector columns: features
[2018-09-07 22:56:36,243 - INFO - koios.io.io]: Writing tsv: data/single_cell_samplesssss
[2018-09-07 22:56:39,086 - INFO - __main__]: Stopping Spark context
[2018-09-07 23:08:20,769 - INFO - __main__]: Initializing pyspark session
[2018-09-07 23:08:25,901 - INFO - __main__]: Config: spark.driver.memory, value: 3G
[2018-09-07 23:08:25,901 - INFO - __main__]: Config: spark.master, value: spark://bs-mbpr134.d.ethz.ch:7077
[2018-09-07 23:08:25,901 - INFO - __main__]: Config: spark.executor.memory, value: 6G
[2018-09-07 23:08:25,901 - INFO - __main__]: Config: spark.app.id, value: app-20180907230824-0016
[2018-09-07 23:08:25,901 - INFO - __main__]: Config: spark.files, value: file:/Users/simondi/PROJECTS/target_infect_x_project/src/tix-analysis/koios/sampler.py
[2018-09-07 23:08:25,901 - INFO - __main__]: Config: spark.rdd.compress, value: True
[2018-09-07 23:08:25,901 - INFO - __main__]: Config: spark.app.name, value: sampler.py
[2018-09-07 23:08:25,901 - INFO - __main__]: Config: spark.serializer.objectStreamReset, value: 100
[2018-09-07 23:08:25,902 - INFO - __main__]: Config: spark.executor.id, value: driver
[2018-09-07 23:08:25,902 - INFO - __main__]: Config: spark.submit.deployMode, value: client
[2018-09-07 23:08:25,902 - INFO - __main__]: Config: spark.driver.host, value: bs-mbpr134.default
[2018-09-07 23:08:25,902 - INFO - __main__]: Config: spark.driver.port, value: 60619
[2018-09-07 23:08:25,903 - INFO - __main__]: Found folder, expecting parquet file as input
[2018-09-07 23:08:25,903 - INFO - __main__]: Found suffix 'tsv', writing RDDs as tsvs
[2018-09-07 23:08:25,903 - INFO - koios.io.io]: Reading parquet folder: data/factor_analysis
[2018-09-07 23:08:47,238 - INFO - koios.io.io]: Writing tsv: data/single_cell_samplesssss
[2018-09-07 23:08:47,994 - ERROR - __main__]: Some error: An error occurred while calling o70.csv.
: java.lang.UnsupportedOperationException: CSV data source does not support struct<type:tinyint,size:int,indices:array<int>,values:array<double>> data type.
	at org.apache.spark.sql.execution.datasources.csv.CSVUtils$.org$apache$spark$sql$execution$datasources$csv$CSVUtils$$verifyType$1(CSVUtils.scala:127)
	at org.apache.spark.sql.execution.datasources.csv.CSVUtils$$anonfun$verifySchema$1.apply(CSVUtils.scala:131)
	at org.apache.spark.sql.execution.datasources.csv.CSVUtils$$anonfun$verifySchema$1.apply(CSVUtils.scala:131)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)
	at org.apache.spark.sql.execution.datasources.csv.CSVUtils$.verifySchema(CSVUtils.scala:131)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.prepareWrite(CSVFileFormat.scala:65)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:140)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:154)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)
	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:642)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)

[2018-09-07 23:08:47,994 - INFO - __main__]: Stopping Spark context
