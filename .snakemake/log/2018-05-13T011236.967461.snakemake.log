Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	1	clustering_fit
	1	clustering_transform
	1	fa
	1	fa_sample
	1	fa_sample_plot
	1	outliers
	7

rule outliers:
    input: data/fa.log
    output: data/outlier-removal.log
    jobid: 3

Finished job 3.
1 of 7 steps (14%) done

rule clustering_fit:
    input: data/outlier-removal.log
    output: data/kmeans-fit.log
    jobid: 6

Finished job 6.
2 of 7 steps (29%) done

rule fa_sample:
    input: data/fa.log
    output: data/fa-sample.log
    jobid: 5

Finished job 5.
3 of 7 steps (43%) done

rule fa:
    input: data/single_cell_samples.tsv
    output: data/fa
    jobid: 2

Terminating processes on user request, this might take some time.
Removing output files of failed job fa since they might be corrupted:
data/fa
[Errno 66] Directory not empty: 'data/fa'
Cancelling snakemake on user request.
